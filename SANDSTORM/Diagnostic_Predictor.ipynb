{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba559b7-45bd-4c17-b9c5-8825693d1b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1,\"../src\")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import GA_util\n",
    "import util as util\n",
    "import nupack as n\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3d5569-5702-4ab0-86a5-3fbf24902169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data = pd.read_csv('../data/CRISPR_data/CCF_merged_pairs_annotated.curated.resampled.tsv',sep='\\t')\n",
    "data = pd.read_csv('../data/CRISPR_data/CCF_merged_pairs_annotated.curated.median.tsv',sep='\\t')\n",
    "\n",
    "# seqs = seqs[seqs['out_logk_measurement'] >-4]\n",
    "\n",
    "data = data[data['type'] == 'exp']\n",
    "\n",
    "values = data['out_logk_measurement'].values\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Guide RNA Activity')\n",
    "plt.hist(values,alpha=0.4,color='coral',density=False)\n",
    "plt.subplot().spines['right'].set_visible(False)\n",
    "plt.subplot().spines['top'].set_visible(False)\n",
    "plt.subplot().spines['left'].set_linewidth(2.0)\n",
    "plt.subplot().spines['bottom'].set_linewidth(2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd9a1312-9ee1-4c7f-99f4-68b1d5ec95e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19124, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c6388d3-199c-4cb0-aa62-c698a8b6b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using resampled version of data to ensure not data leakage\n",
    "cat_save = []\n",
    "start = 0\n",
    "for i in range(data.shape[0]):\n",
    "\n",
    "    if i %10==0:\n",
    "        start +=1\n",
    "        cat_save.append(start)\n",
    "    else:\n",
    "        cat_save.append(start)\n",
    "            \n",
    "data['category'] = cat_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f99ff299-5142-4ecf-8cf1-ba5c21cfb7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/GARDN/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:279: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3825., 1912., 1913., 1912.,    0., 1912., 1913., 1912., 1912.,\n",
       "        1913.]),\n",
       " array([0. , 0.8, 1.6, 2.4, 3.2, 4. , 4.8, 5.6, 6.4, 7.2, 8. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "est =  KBinsDiscretizer(n_bins=10,encode='ordinal',strategy='quantile')\n",
    "bins = est.fit(values.reshape(-1,1))\n",
    "encoded_vals = bins.transform(values.reshape(-1,1))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(encoded_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81df65d3-3b94-4a36-ac94-63aa5658ab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19124, 4, 68)\n"
     ]
    }
   ],
   "source": [
    "guides = util.one_hot_encode(pd.DataFrame((data[['guide_seq']])))\n",
    "\n",
    "guides_extended = np.concatenate((np.zeros(shape=(guides.shape[0],4,20)),\n",
    "                                  guides,\n",
    "                                 np.zeros(shape=(guides.shape[0],4,20))),\n",
    "                                axis=2)\n",
    "\n",
    "print(guides_extended.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6242a88a-e683-481e-a656-587010bffc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_before = util.one_hot_encode(pd.DataFrame(data[['target_before']]))\n",
    "targets_middle = util.one_hot_encode(pd.DataFrame(data[['target_at_guide']]))\n",
    "targets_after = util.one_hot_encode(pd.DataFrame(data[['target_after']]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acb10c76-b53a-43dd-92b9-aef5c8d6ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seqs = np.concatenate((targets_before[:,:,:],targets_middle,targets_after[:,:,:]),axis=2)\n",
    "# sandstorm_seqs = util.one_hot_encode(targets)\n",
    "ppms = GA_util.prototype_ppms_fast(target_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa05508-8946-40b4-ac84-97330a963ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19124, 8, 68)\n"
     ]
    }
   ],
   "source": [
    "seqs = np.concatenate((target_seqs,guides_extended),axis=1)\n",
    "# seqs = target_seqs\n",
    "\n",
    "print(seqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af36c833-d3e7-458b-b15f-99229301504c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15294, 8, 68)\n",
      "(3830, 8, 68)\n",
      "Metal device set to: Apple M1 Ultra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:47:24.437079: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-03-18 18:47:24.437102: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"joint_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 68, 68, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 1, 68, 16)    1312        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 8, 68, 1)]   0           []                               \n",
      "                                                                                                  \n",
      " spatial_dropout2d (SpatialDrop  (None, 1, 68, 16)   0           ['conv2d_3[0][0]']               \n",
      " out2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 1, 68, 16)    2320        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 1, 68, 8)     3208        ['spatial_dropout2d[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 1, 68, 16)   64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 1, 68, 4)     292         ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 1, 68, 8)     9224        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " global_max_pooling2d (GlobalMa  (None, 4)           0           ['conv2d_5[0][0]']               \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 1, 68, 4)     772         ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 4)            0           ['global_max_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 272)          0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 276)          0           ['flatten_1[0][0]',              \n",
      "                                                                  'flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           4432        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 8)            136         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4)            36          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " prediction_output_0 (Dense)    (None, 1)            5           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,801\n",
      "Trainable params: 21,769\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "TRAINING SANDSTORM\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:47:25.536985: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2025-03-18 18:47:25.908340: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - ETA: 0s - loss: 2.3050"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:47:33.795664: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 9s 14ms/step - loss: 2.3050 - val_loss: 2.3020\n",
      "Epoch 2/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 1.1618 - val_loss: 1.3430\n",
      "Epoch 3/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 1.0757 - val_loss: 1.1786\n",
      "Epoch 4/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 1.0154 - val_loss: 0.9863\n",
      "Epoch 5/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.9475 - val_loss: 0.9737\n",
      "Epoch 6/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.8412 - val_loss: 0.8735\n",
      "Epoch 7/20\n",
      "239/239 [==============================] - 2s 8ms/step - loss: 0.6883 - val_loss: 0.6485\n",
      "Epoch 8/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.5714 - val_loss: 0.5789\n",
      "Epoch 9/20\n",
      "239/239 [==============================] - 2s 8ms/step - loss: 0.5284 - val_loss: 0.5470\n",
      "Epoch 10/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.5036 - val_loss: 0.5345\n",
      "Epoch 11/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4823 - val_loss: 0.5252\n",
      "Epoch 12/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4691 - val_loss: 0.5138\n",
      "Epoch 13/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4547 - val_loss: 0.6243\n",
      "Epoch 14/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4421 - val_loss: 0.5202\n",
      "Epoch 15/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4327 - val_loss: 0.4987\n",
      "Epoch 16/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4230 - val_loss: 0.4930\n",
      "Epoch 17/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4160 - val_loss: 0.5417\n",
      "Epoch 18/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4062 - val_loss: 0.4892\n",
      "Epoch 19/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.3981 - val_loss: 0.4804\n",
      "Epoch 20/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.3937 - val_loss: 0.4784\n",
      " 39/120 [========>.....................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:48:13.989326: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 3ms/step\n",
      "0.643468557397337\n",
      "(15294, 8, 68)\n",
      "(3830, 8, 68)\n",
      "Model: \"joint_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 68, 68, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 1, 68, 16)    1312        ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 8, 68, 1)]   0           []                               \n",
      "                                                                                                  \n",
      " spatial_dropout2d_1 (SpatialDr  (None, 1, 68, 16)   0           ['conv2d_9[0][0]']               \n",
      " opout2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 1, 68, 16)    2320        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 1, 68, 8)     3208        ['spatial_dropout2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 1, 68, 16)   64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 1, 68, 4)     292         ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 1, 68, 8)     9224        ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " global_max_pooling2d_1 (Global  (None, 4)           0           ['conv2d_11[0][0]']              \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 1, 68, 4)     772         ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 4)            0           ['global_max_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 272)          0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 276)          0           ['flatten_3[0][0]',              \n",
      "                                                                  'flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 16)           4432        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 8)            136         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4)            36          ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " prediction_output_0 (Dense)    (None, 1)            5           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,801\n",
      "Trainable params: 21,769\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "TRAINING SANDSTORM\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:48:14.983989: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - ETA: 0s - loss: 2.1991"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:48:17.376136: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 3s 10ms/step - loss: 2.1991 - val_loss: 2.5786\n",
      "Epoch 2/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 1.1532 - val_loss: 1.4154\n",
      "Epoch 3/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 1.0664 - val_loss: 1.0459\n",
      "Epoch 4/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.9859 - val_loss: 0.9438\n",
      "Epoch 5/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.8722 - val_loss: 0.8924\n",
      "Epoch 6/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.7109 - val_loss: 0.7294\n",
      "Epoch 7/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.6207 - val_loss: 0.5926\n",
      "Epoch 8/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.5760 - val_loss: 0.5687\n",
      "Epoch 9/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.5457 - val_loss: 0.5442\n",
      "Epoch 10/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.5251 - val_loss: 0.5368\n",
      "Epoch 11/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.5071 - val_loss: 0.5161\n",
      "Epoch 12/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4910 - val_loss: 0.5132\n",
      "Epoch 13/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4789 - val_loss: 0.4989\n",
      "Epoch 14/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4667 - val_loss: 0.4949\n",
      "Epoch 15/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4586 - val_loss: 0.4882\n",
      "Epoch 16/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4541 - val_loss: 0.4992\n",
      "Epoch 17/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4400 - val_loss: 0.4777\n",
      "Epoch 18/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4333 - val_loss: 0.4779\n",
      "Epoch 19/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4318 - val_loss: 0.5364\n",
      "Epoch 20/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4235 - val_loss: 0.4670\n",
      " 38/120 [========>.....................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:48:57.889197: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 3ms/step\n",
      "0.6523424030738953\n",
      "(15294, 8, 68)\n",
      "(3830, 8, 68)\n",
      "Model: \"joint_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 68, 68, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 1, 68, 16)    1312        ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 8, 68, 1)]   0           []                               \n",
      "                                                                                                  \n",
      " spatial_dropout2d_2 (SpatialDr  (None, 1, 68, 16)   0           ['conv2d_15[0][0]']              \n",
      " opout2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 1, 68, 16)    2320        ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 1, 68, 8)     3208        ['spatial_dropout2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 1, 68, 16)   64          ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 1, 68, 4)     292         ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 1, 68, 8)     9224        ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " global_max_pooling2d_2 (Global  (None, 4)           0           ['conv2d_17[0][0]']              \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 1, 68, 4)     772         ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 4)            0           ['global_max_pooling2d_2[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 272)          0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 276)          0           ['flatten_5[0][0]',              \n",
      "                                                                  'flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           4432        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 8)            136         ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4)            36          ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " prediction_output_0 (Dense)    (None, 1)            5           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,801\n",
      "Trainable params: 21,769\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "TRAINING SANDSTORM\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:48:58.866097: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - ETA: 0s - loss: 2.8547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:49:01.269917: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 3s 10ms/step - loss: 2.8547 - val_loss: 2.5404\n",
      "Epoch 2/20\n",
      "239/239 [==============================] - 2s 8ms/step - loss: 1.1945 - val_loss: 1.4520\n",
      "Epoch 3/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 1.1081 - val_loss: 1.1413\n",
      "Epoch 4/20\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 1.0334 - val_loss: 1.0462\n",
      "Epoch 5/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.9481 - val_loss: 0.9671\n",
      "Epoch 6/20\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.8116 - val_loss: 0.8667\n",
      "Epoch 7/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.6352 - val_loss: 0.6789\n",
      "Epoch 8/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.5571 - val_loss: 0.5832\n",
      "Epoch 9/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.5299 - val_loss: 0.5752\n",
      "Epoch 10/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.5116 - val_loss: 0.5553\n",
      "Epoch 11/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4914 - val_loss: 0.5417\n",
      "Epoch 12/20\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.4784 - val_loss: 0.5473\n",
      "Epoch 13/20\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.4695 - val_loss: 0.5457\n",
      "Epoch 14/20\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.4529 - val_loss: 0.5251\n",
      "Epoch 15/20\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.4459 - val_loss: 0.5877\n",
      "Epoch 16/20\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.4356 - val_loss: 0.5061\n",
      "Epoch 17/20\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.4244 - val_loss: 0.5286\n",
      "Epoch 18/20\n",
      "239/239 [==============================] - 2s 10ms/step - loss: 0.4191 - val_loss: 0.5689\n",
      "Epoch 19/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4102 - val_loss: 0.5425\n",
      "Epoch 20/20\n",
      "239/239 [==============================] - 2s 9ms/step - loss: 0.4032 - val_loss: 0.4855\n",
      " 34/120 [=======>......................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 18:49:44.661692: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 1s 3ms/step\n",
      "0.6416040985750213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split,GroupShuffleSplit\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr\n",
    "import random\n",
    "import keras\n",
    "\n",
    "\n",
    "# seeds = [1221,4546,9999]\n",
    "loss = 'mse'\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "seeds = [1221,4546,999]\n",
    "folds = 3\n",
    "\n",
    "\n",
    "joint_r2_save = []\n",
    "joint_spearman_save = []\n",
    "joint_mse_save = []\n",
    "\n",
    "seq_mse_save = []\n",
    "seq_r2_save = []\n",
    "seq_spearman_save = []\n",
    "\n",
    "\n",
    "for i in range(folds):\n",
    "    \n",
    "\n",
    "    # seq_train,seq_test,ppm_train,ppm_test,y_train,y_test = train_test_split(seqs,ppms,values,stratify=encoded_vals)\n",
    "    gss = GroupShuffleSplit(n_splits=1,test_size=0.2,random_state=seeds[i])\n",
    "    \n",
    "    train_idx,test_idx = next(gss.split(data,groups=data['category']))\n",
    "\n",
    "    # seq_train,seq_test,ppm_train,ppm_test,y_train,y_test = GroupShuffleSplit(n_split=1,sandstorm_seqs,ppms,values,group_column='category')\n",
    "    \n",
    "    seq_train = seqs[train_idx,:,:]\n",
    "    seq_test = seqs[test_idx,:,:]\n",
    "    \n",
    "    ppm_train = ppms[train_idx,:,:]\n",
    "    ppm_test = ppms[test_idx,:,:]\n",
    "    \n",
    "    y_train = values[train_idx]\n",
    "    y_test = values[test_idx]\n",
    "\n",
    "    print(seq_train.shape)\n",
    "    print(seq_test.shape)\n",
    "\n",
    "    joint_model = GA_util.create_SANDSTORM_cas(seq_len=seqs.shape[2],ppm_len=ppms.shape[2],latent_dim=64,internal_activation='relu',output_activation='linear')\n",
    "    print(joint_model.summary())\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    joint_model.compile(loss=loss,optimizer=optimizer)\n",
    "    \n",
    "# es = tfk.callbacks.EarlyStopping(monitor='val_loss',\n",
    "#             mode='min', patience=10,restore_best_weights=True)\n",
    "\n",
    "\n",
    "    print(\"TRAINING SANDSTORM\")\n",
    "    hist = joint_model.fit([seq_train,ppm_train],y_train,\n",
    "                           validation_data=[[seq_test,ppm_test],y_test],\n",
    "                           epochs=epochs,\n",
    "                           batch_size=batch_size)    \n",
    "\n",
    "\n",
    "\n",
    "    y_preds = joint_model.predict([seq_test,ppm_test])\n",
    "    joint_mse =hist.history['val_loss'][-1]\n",
    "    joint_r2 = r2_score(y_test,y_preds)\n",
    "    joint_spearman = spearmanr(y_test,y_preds)[0]\n",
    "    \n",
    "    joint_mse_save.append(joint_mse)\n",
    "    joint_r2_save.append(joint_r2)\n",
    "    joint_spearman_save.append(joint_spearman)\n",
    "    print(r2_score(y_test,y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c8571f2-fe41-4b9d-941d-9c356a076c55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "\n",
    "class Cas13ActivityParser:\n",
    "    \"\"\"Parse data from paired crRNA/target Cas13 data tested with CARMEN.\n",
    "    The output is numeric values (for regression) rather than labels.\n",
    "    \"\"\"\n",
    "    INPUT_TSV_RESAMPLED = '../data/CRISPR_data/CCF_merged_pairs_annotated.curated.resampled.tsv.gz'\n",
    "    INPUT_TSV_MEDIAN = '../data/CRISPR_data/CCF_merged_pairs_annotated.curated.median.tsv.gz'\n",
    "\n",
    "    # Define crRNA (guide) length; used for determining range of crRNA\n",
    "    # in nucleotide space\n",
    "    CRRNA_LEN = 28\n",
    "\n",
    "    # Define the seed region; for Cas13a, the middle ~third of the spacer\n",
    "    SEED_START = int(CRRNA_LEN * 1/3) # 0-based, inclusive\n",
    "    SEED_END = int(CRRNA_LEN * 2/3) + 1 # 0-based, exclusive\n",
    "\n",
    "    # Define threshold on activity for inactive/active data points\n",
    "    ACTIVITY_THRESHOLD = -4.0\n",
    "\n",
    "    def __init__(self, subset=None, context_nt=10, split=(0.8, 0.1, 0.1),\n",
    "            shuffle_seed=1, stratify_randomly=False, stratify_by_pos=False,\n",
    "            use_median_measurement=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            subset: either 'exp' (use only experimental data points, which\n",
    "                generally have a mismatch between guide/target), 'pos' (use\n",
    "                only data points corresponding to a positive guide/target\n",
    "                match with no mismatches (i.e., the wildtype target)),\n",
    "                'neg' (use only data points corresponding to negative guide/\n",
    "                target (i.e., high divergence between the two)), or\n",
    "                'exp-and-pos;; if 'None', use all data points\n",
    "            context_nt: nt of target sequence context to include alongside\n",
    "                each guide\n",
    "            split: (train, validation, test) split; must sum to 1.0\n",
    "            shuffle_seed: seed to use for the random module to shuffle\n",
    "            stratify_randomly: if set, shuffle rows before splitting into\n",
    "                train/validate/test\n",
    "            stratify_by_pos: if set, consider the position along the target\n",
    "                and split based on this\n",
    "            use_median_measurement: if True, use median of replicate measurements;\n",
    "                otherwise, use resampled values\n",
    "        \"\"\"\n",
    "        assert subset in (None, 'exp', 'pos', 'neg', 'exp-and-pos')\n",
    "        self.subset = subset\n",
    "\n",
    "        self.context_nt = context_nt\n",
    "\n",
    "        assert sum(split) == 1.0\n",
    "        self.split_train, self.split_validate, self.split_test = split\n",
    "\n",
    "        if stratify_randomly and stratify_by_pos:\n",
    "            raise ValueError(\"Cannot set by stratify_randomly and stratify_by_pos\")\n",
    "        self.stratify_randomly = stratify_randomly\n",
    "        self.stratify_by_pos = stratify_by_pos\n",
    "\n",
    "        random.seed(shuffle_seed)\n",
    "\n",
    "        self.classify_activity = False\n",
    "        self.regress_on_all = False\n",
    "        self.regress_only_on_active = False\n",
    "\n",
    "        self.make_feats_for_baseline = None\n",
    "\n",
    "        self.normalize_crrna_activity = False\n",
    "        self.use_difference_from_wildtype_activity = False\n",
    "\n",
    "        self.was_read = False\n",
    "\n",
    "        if use_median_measurement:\n",
    "            self.INPUT_TSV = self.INPUT_TSV_MEDIAN\n",
    "        else:\n",
    "            self.INPUT_TSV = self.INPUT_TSV_RESAMPLED\n",
    "\n",
    "    def set_activity_mode(self, classify_activity, regress_on_all,\n",
    "            regress_only_on_active):\n",
    "        \"\"\"Set mode for which points to read regarding their activity.\n",
    "        Args:\n",
    "            classify_activity: if True, have the output variable be a label\n",
    "                (False/True) regarding activity of a guide/target pair\n",
    "            regress_on_all: if True, output all guide/target pairs\n",
    "            regress_only_on_active: if True, only output guide/target pairs\n",
    "                corresponding to high activity\n",
    "        \"\"\"\n",
    "        num_set = (int(classify_activity) + int(regress_on_all) +\n",
    "                int(regress_only_on_active))\n",
    "        if num_set != 1:\n",
    "            raise Exception((\"Exactly one of 'classify_activity' and \"\n",
    "                \"'regress_on_all' and 'regress_only_on_active' can be set\"))\n",
    "        self.classify_activity = classify_activity\n",
    "        self.regress_on_all = regress_on_all\n",
    "        self.regress_only_on_active = regress_only_on_active\n",
    "\n",
    "    def set_make_feats_for_baseline(self, feats):\n",
    "        \"\"\"Generate input features specifically for the baseline model.\n",
    "        Args:\n",
    "            feats: one of 'onehot-flat' (one-hot encoding, flattened to 1D);\n",
    "                'onehot-simple' (one-hot encoding that encodes the target\n",
    "                sequence and mismatches between the guide and it, leaving the\n",
    "                guide encoding as all 0 when matching); 'handcrafted'\n",
    "                (nucleotide frequency, dinucleotide frequency, etc.); or\n",
    "                'combined' (concatenated 'onehot-simple' and 'handcrafted')\n",
    "        \"\"\"\n",
    "        self.make_feats_for_baseline = feats\n",
    "\n",
    "    def set_normalize_crrna_activity(self):\n",
    "        \"\"\"Normalize activity of each crRNA, across targets, to have mean 0 and\n",
    "        stdev 1.\n",
    "        We can only set one of 'normalize_crrna_activity' and\n",
    "        'use_difference_from_wildtype_activity'.\n",
    "        \"\"\"\n",
    "        assert self.use_difference_from_wildtype_activity is False\n",
    "        self.normalize_crrna_activity = True\n",
    "\n",
    "    def set_use_difference_from_wildtype_activity(self):\n",
    "        \"\"\"Use, as the activity value for a pair of guide g and target t, the\n",
    "        difference between the g-t activity and the mean activity between g and\n",
    "        its wildtype (matching) targets.\n",
    "        We can only set one of 'normalize_crrna_activity' and\n",
    "        'use_difference_from_wildtype_activity'.\n",
    "        \"\"\"\n",
    "        assert self.normalize_crrna_activity is False\n",
    "        self.use_difference_from_wildtype_activity = True\n",
    "\n",
    "    def _gen_input_and_output(self, row):\n",
    "        \"\"\"Generate input features and output for each row.\n",
    "        This generates a one-hot encoding for each sequence. Because we have\n",
    "        the target ('target_at_guide') and guide sequence ('guide_seq'),\n",
    "        we must encode how they compare to each other. Here, for each nucleotide\n",
    "        position, we use an 8-bit vector (4 to encode the target sequence and\n",
    "        4 for the guide sequence). For example, 'A' in the target and 'G' in the\n",
    "        guide will be [1,0,0,0,0,0,1,0] for [A,C,G,T] one-hot encoding. There are\n",
    "        other ways to do this as well: e.g., a 4-bit vector that represents an OR\n",
    "        between the one-hot encoding of the target and guide (e.g., 'A' in the\n",
    "        target and 'G' in the guide would be [1,0,1,0]), but this does not\n",
    "        distinguish between bases in the target and guide (i.e., the encoding\n",
    "        is the same for 'G' in the target and 'A' in the guide) which might\n",
    "        be important here.\n",
    "        Note that when self.make_feats_for_baseline is set, the input feature\n",
    "        vector is different, depending on its value.\n",
    "        Args:\n",
    "            row: dict representing row of data (key'd by column\n",
    "                name)\n",
    "        Returns:\n",
    "            tuple (i, out) where i is a one-hot encoding of the input\n",
    "            and out is an output value (or out is in the format specified by\n",
    "            self.make_feats_for_baseline)\n",
    "        \"\"\"\n",
    "        # Check self.make_feats_for_baseline\n",
    "        assert self.make_feats_for_baseline in [None, 'onehot-flat',\n",
    "            'onehot-simple', 'handcrafted', 'combined']\n",
    "\n",
    "        onehot_idx = {'A': 0, 'G': 1, 'C': 2, 'T': 3}\n",
    "        onehot_order = ('A', 'G', 'C', 'T')\n",
    "        def onehot(b):\n",
    "            # One-hot encoding of base b\n",
    "            assert b in onehot_idx.keys()\n",
    "            v = [0, 0, 0, 0]\n",
    "            v[onehot_idx[b]] = 1\n",
    "            return v\n",
    "\n",
    "        # For the baseline, keep a short string description of the\n",
    "        # feature at each index\n",
    "        if self.make_feats_for_baseline in ['onehot-flat', 'onehot-simple',\n",
    "                'handcrafted', 'combined']:\n",
    "            self.baseline_descriptions = []\n",
    "        else:\n",
    "            self.baseline_descriptions = None\n",
    "        def descriptions_for_onehot(s):\n",
    "            # List of descriptions for each base\n",
    "            return [s + '-' + b for b in onehot_order]\n",
    "\n",
    "        # Create the input features for target sequence context on\n",
    "        # the end before the guide\n",
    "        input_feats_context_before = []\n",
    "        context_before = row['target_before']\n",
    "        assert self.context_nt <= len(context_before)\n",
    "        start = len(context_before) - self.context_nt\n",
    "        for pos in range(start, len(context_before)):\n",
    "            # Make a one-hot encoding for this position of the target sequence\n",
    "            v = onehot(context_before[pos])\n",
    "            if self.make_feats_for_baseline is None:\n",
    "                # For the 4 bits of guide sequence, use [0,0,0,0] (there is\n",
    "                # no guide at this position)\n",
    "                v += [0, 0, 0, 0]\n",
    "                input_feats_context_before += [v]\n",
    "            elif self.make_feats_for_baseline in ['onehot-flat',\n",
    "                    'onehot-simple', 'combined']:\n",
    "                # For the baseline, only use a one-hot encoding of the\n",
    "                # target (and in a 1D array)\n",
    "                input_feats_context_before += v\n",
    "                self.baseline_descriptions += descriptions_for_onehot(\n",
    "                        'target-before-'+str(pos))\n",
    "            elif self.make_feats_for_baseline == 'handcrafted':\n",
    "                # No feature for the baseline here\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"Unknown choice of make_feats_for_baseline\")\n",
    "\n",
    "        # Create the input features for target and guide sequence\n",
    "        input_feats_guide = []\n",
    "        target = row['target_at_guide']\n",
    "        guide = row['guide_seq']\n",
    "        baseline_mismatches_pos = []\n",
    "        assert len(target) == len(guide)\n",
    "        for pos in range(len(guide)):\n",
    "            # Make a one-hot encoding (4 bits) for each of the target\n",
    "            # and the guide\n",
    "            v_target = onehot(target[pos])\n",
    "            v_guide = onehot(guide[pos])\n",
    "            if self.make_feats_for_baseline is None:\n",
    "                # Combine them into an 8-bit vector\n",
    "                v = v_target + v_guide\n",
    "                input_feats_guide += [v]\n",
    "            elif self.make_feats_for_baseline == 'onehot-flat':\n",
    "                # For the baseline, use an 8-bit vector (flattened - i.e.,\n",
    "                # concatenated with the other positions)\n",
    "                v = v_target + v_guide\n",
    "                input_feats_guide += v\n",
    "                self.baseline_descriptions += descriptions_for_onehot(\n",
    "                        'target-at-guide-'+str(pos))\n",
    "                self.baseline_descriptions += descriptions_for_onehot(\n",
    "                        'guide-seq-'+str(pos))\n",
    "            elif self.make_feats_for_baseline in ['onehot-simple', 'combined']:\n",
    "                # For the baseline, use a one-hot encoding of the target\n",
    "                # (in a 1D array) and then use a one-hot encoding that gives\n",
    "                # whether there is a mismatch and, if so, what the guide\n",
    "                # base is\n",
    "                if target[pos] == guide[pos]:\n",
    "                    # No mismatch; use 0,0,0,0 for the guide\n",
    "                    input_feats_guide += v_target + [0, 0, 0, 0]\n",
    "                else:\n",
    "                    # Mismatch; have the guide indicate which base there is\n",
    "                    input_feats_guide += v_target + v_guide\n",
    "                    baseline_mismatches_pos += [pos]\n",
    "                self.baseline_descriptions += descriptions_for_onehot(\n",
    "                        'target-at-guide-'+str(pos))\n",
    "                self.baseline_descriptions += descriptions_for_onehot(\n",
    "                        'guide-mismatch-allele-'+str(pos))\n",
    "            elif self.make_feats_for_baseline == 'handcrafted':\n",
    "                # Mark number of mismatches, but do not use input_feats_guide\n",
    "                if target[pos] != guide[pos]:\n",
    "                    baseline_mismatches_pos += [pos]\n",
    "            else:\n",
    "                raise ValueError(\"Unknown choice of make_feats_for_baseline\")\n",
    "    \n",
    "        # Create the input features for target sequence context on\n",
    "        # the end after the guide\n",
    "        input_feats_context_after = []\n",
    "        context_after = row['target_after']\n",
    "        assert self.context_nt <= len(context_after)\n",
    "        for pos in range(self.context_nt):\n",
    "            # Make a one-hot encoding for this position of the target sequence\n",
    "            v = onehot(context_after[pos])\n",
    "            if self.make_feats_for_baseline is None:\n",
    "                # For the 4 bits of guide sequence, use [0,0,0,0] (there is\n",
    "                # no guide at this position)\n",
    "                v += [0, 0, 0, 0]\n",
    "                input_feats_context_after += [v]\n",
    "            elif self.make_feats_for_baseline in ['onehot-flat',\n",
    "                    'onehot-simple', 'combined']:\n",
    "                # For the baseline, only use a one-hot encoding of the\n",
    "                # target (and in a 1D array)\n",
    "                if self.make_feats_for_baseline == 'combined' and pos in [0, 1]:\n",
    "                    # We add a PFS feature, with 2 nt, below; so do not\n",
    "                    # add features here (this would just decouple the 2 nt in\n",
    "                    # that feature)\n",
    "                    pass\n",
    "                else:\n",
    "                    input_feats_context_after += v\n",
    "                    self.baseline_descriptions += descriptions_for_onehot(\n",
    "                            'target-after-'+str(pos))\n",
    "            elif self.make_feats_for_baseline == 'handcrafted':\n",
    "                # No feature for the baseline here\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"Unknown choice of make_feats_for_baseline\")\n",
    "\n",
    "        # Combine the input features\n",
    "        input_feats = input_feats_context_before + input_feats_guide + input_feats_context_after\n",
    "        if self.make_feats_for_baseline == 'handcrafted':\n",
    "            # The features directly from sequence should not have been set\n",
    "            assert len(input_feats) == 0\n",
    "        if self.make_feats_for_baseline in ['handcrafted', 'combined']:\n",
    "            # Have the feature vector for the baseline include additional\n",
    "            # features: position-independent nucleotide frequency, dinucleotide\n",
    "            # frequency, and GC content (all in the guide); and number of\n",
    "            # mismatches between guide and target\n",
    "            bases = ('A', 'G', 'C', 'T')\n",
    "            for b in bases:\n",
    "                # Add a feature giving nucleotide frequency (count) of b in\n",
    "                # the guide\n",
    "                input_feats += [guide.count(b)]\n",
    "                self.baseline_descriptions += ['nucleotide-count-'+b]\n",
    "            for b1 in bases:\n",
    "                for b2 in bases:\n",
    "                    # Add a feature giving dinucleotide frequency (count) of\n",
    "                    # b1+b2 in the guide\n",
    "                    input_feats += [guide.count(b1 + b2)]\n",
    "                    self.baseline_descriptions += ['dinucleotide-count-'+b1+b2]\n",
    "            # Add a feature giving GC count in the guide\n",
    "            input_feats += [guide.count('G') + guide.count('C')]\n",
    "            self.baseline_descriptions += ['gc-count']\n",
    "            # Add a feature giving number of mismatches outside the seed region\n",
    "            # and in the seed\n",
    "            #seed_num_mismatches = len([p for p in baseline_mismatches_pos\n",
    "            #    if p >= self.SEED_START and p < self.SEED_END])\n",
    "            #nonseed_num_mismatches = (len(baseline_mismatches_pos) -\n",
    "            #    seed_num_mismatches)\n",
    "            #input_feats += [seed_num_mismatches]\n",
    "            #self.baseline_descriptions += ['num-mismatches-seed']\n",
    "            #input_feats += [nonseed_num_mismatches]\n",
    "            #self.baseline_descriptions += ['num-mismatches-nonseed']\n",
    "            total_num_mismatches = len(baseline_mismatches_pos)\n",
    "            input_feats += [total_num_mismatches]\n",
    "            self.baseline_descriptions += ['num-mismatches']\n",
    "            # Add PFS with 2 nt (canonical PFS and 1 nt after), one-hot\n",
    "            # encoded\n",
    "            # This essentially couples the target-after-0-* and\n",
    "            # target-after-1-* features, rather than leaving them decoupled\n",
    "            pfs2 = context_after[0:2]\n",
    "            for b1 in bases:\n",
    "                for b2 in bases:\n",
    "                    oh = (pfs2 == b1 + b2)\n",
    "                    input_feats += [oh]\n",
    "                    self.baseline_descriptions += ['pfs-'+b1+b2]\n",
    "        input_feats = np.array(input_feats)\n",
    "\n",
    "        # If baseline_descriptions is being set, check its length\n",
    "        if self.baseline_descriptions is not None:\n",
    "            assert len(self.baseline_descriptions) == len(input_feats)\n",
    "\n",
    "        # Determine an output for this row\n",
    "        activity = float(row['out_logk_measurement'])\n",
    "        if self.classify_activity:\n",
    "            # Make the output be a 1/0 label\n",
    "\n",
    "            # Let 0 be inactive labels, and 1 be active ones\n",
    "            if activity <= self.ACTIVITY_THRESHOLD:\n",
    "                activity = 0\n",
    "            else:\n",
    "                activity = 1\n",
    "        else:\n",
    "            pos = int(row['guide_pos_nt'])\n",
    "            if self.normalize_crrna_activity:\n",
    "                crrna_mean = self.crrna_activity_mean[pos]\n",
    "                crrna_stdev = self.crrna_activity_stdev[pos]\n",
    "                activity = (activity - crrna_mean) / crrna_stdev\n",
    "            if self.use_difference_from_wildtype_activity:\n",
    "                wildtype_mean = self.crrna_wildtype_activity_mean[pos]\n",
    "                activity = activity - wildtype_mean\n",
    "\n",
    "        return (input_feats, activity)\n",
    "\n",
    "    def read(self):\n",
    "        \"\"\"Read and parse TSV file.\n",
    "        \"\"\"\n",
    "        # Read all rows\n",
    "        header_idx = {}\n",
    "        rows = []\n",
    "        with gzip.open(self.INPUT_TSV, 'rt') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                ls = line.rstrip().split('\\t')\n",
    "                if i == 0:\n",
    "                    # Parse header\n",
    "                    for j in range(len(ls)):\n",
    "                        header_idx[ls[j]] = j\n",
    "                else:\n",
    "                    rows += [ls]\n",
    "\n",
    "        # Convert rows to be key'd by column name\n",
    "        rows_new = []\n",
    "        for row in rows:\n",
    "            row_dict = {k: row[header_idx[k]] for k in header_idx.keys()}\n",
    "            rows_new += [row_dict]\n",
    "        rows = rows_new\n",
    "\n",
    "        if self.subset == 'exp':\n",
    "            # Only keep rows where type is 'exp'\n",
    "            rows = [row for row in rows if row['type'] == 'exp']\n",
    "        if self.subset == 'pos':\n",
    "            # Only keep rows where type is 'pos'\n",
    "            rows = [row for row in rows if row['type'] == 'pos']\n",
    "        if self.subset == 'neg':\n",
    "            # Only keep rows where type is 'neg'\n",
    "            rows = [row for row in rows if row['type'] == 'neg']\n",
    "        if self.subset == 'exp-and-pos':\n",
    "            # Only keep rows where type is 'exp' or 'pos'\n",
    "            rows = [row for row in rows if row['type'] == 'exp' or\n",
    "                    row['type'] == 'pos']\n",
    "\n",
    "        # Shuffle the rows before splitting\n",
    "        if self.stratify_randomly:\n",
    "            random.shuffle(rows)\n",
    "\n",
    "        # Sort by position before splitting\n",
    "        if self.stratify_by_pos:\n",
    "            rows = sorted(rows, key=lambda x: float(x['guide_pos_nt']))\n",
    "\n",
    "        # Remove the inactive points\n",
    "        if self.regress_only_on_active:\n",
    "            rows = [row for row in rows if\n",
    "                    float(row['out_logk_measurement']) > self.ACTIVITY_THRESHOLD]\n",
    "        # Calculate the mean and stdev of activity for each crRNA (according\n",
    "        # to position); note that the input includes multiple measurements\n",
    "        # (technical replicates) for each, so these statistics are taken across\n",
    "        # the sampled measurements\n",
    "        # Note that this is only used for regression, so only add it for the\n",
    "        # active guide-target pairs\n",
    "        activity_by_pos = defaultdict(list)\n",
    "        for row in rows:\n",
    "            pos = int(row['guide_pos_nt'])\n",
    "            activity = float(row['out_logk_measurement'])\n",
    "            activity_by_pos[pos].append(activity)\n",
    "        self.crrna_activity_mean = {pos: np.mean(activity_by_pos[pos])\n",
    "                for pos in activity_by_pos.keys()}\n",
    "        self.crrna_activity_stdev = {pos: np.std(activity_by_pos[pos])\n",
    "                for pos in activity_by_pos.keys()}\n",
    "\n",
    "        # For each crRNA (according to positive), calculate the mean activity\n",
    "        # between it and the wildtype targets\n",
    "        wildtype_activity_by_pos = defaultdict(list)\n",
    "        for row in rows:\n",
    "            if int(row['guide_target_hamming_dist']) == 0:\n",
    "                # This is a wildtype target\n",
    "                pos = int(row['guide_pos_nt'])\n",
    "                activity = float(row['out_logk_measurement'])\n",
    "                wildtype_activity_by_pos[pos].append(activity)\n",
    "        self.crrna_wildtype_activity_mean = {pos: np.mean(wildtype_activity_by_pos[pos])\n",
    "                for pos in wildtype_activity_by_pos.keys()}\n",
    "        # Generate input and outputs for each row\n",
    "        inputs_and_outputs = []\n",
    "        self.input_feats_pos = {}\n",
    "        row_idx_pos = []\n",
    "        for row in rows:\n",
    "            pos = int(row['guide_pos_nt'])\n",
    "\n",
    "            # Generate an input feature vector and a (list of) output(s)\n",
    "            input_feats, output = self._gen_input_and_output(row)\n",
    "            inputs_and_outputs += [(input_feats, output)]\n",
    "            row_idx_pos += [pos]\n",
    "\n",
    "            # Store a mapping from the input feature vector to the guide\n",
    "            # position in the library design\n",
    "            input_feats_key = np.array(input_feats, dtype='f').tostring()\n",
    "            if input_feats_key in self.input_feats_pos:\n",
    "                assert self.input_feats_pos[input_feats_key] == pos\n",
    "            else:\n",
    "                self.input_feats_pos[input_feats_key] = pos\n",
    "        # Split into train, validate, and test sets\n",
    "        train_end_idx = int(len(inputs_and_outputs) * self.split_train)\n",
    "        validate_end_idx = int(len(inputs_and_outputs) * (self.split_train + self.split_validate))\n",
    "        self._train_set = []\n",
    "        self._validate_set = []\n",
    "        self._test_set = []\n",
    "        for i in range(len(inputs_and_outputs)):\n",
    "            if i <= train_end_idx:\n",
    "                self._train_set += [inputs_and_outputs[i]]\n",
    "            elif i <= validate_end_idx:\n",
    "                self._validate_set += [inputs_and_outputs[i]]\n",
    "            else:\n",
    "                if self.stratify_by_pos:\n",
    "                    # If there are points with the exact same position that\n",
    "                    # are split across the validate/test sets, then the ones\n",
    "                    # from the test set will get removed because they overlap\n",
    "                    # the validate set; instead of letting these ones get\n",
    "                    # tossed, just put them in the validate set\n",
    "                    last_validate_pos = row_idx_pos[validate_end_idx]\n",
    "                    if row_idx_pos[i] == last_validate_pos:\n",
    "                        if self.split_validate == 0:\n",
    "                            # There should be no validate set; put it in train\n",
    "                            # set instead\n",
    "                            self._train_set += [inputs_and_outputs[i]]\n",
    "                        else:\n",
    "                            self._validate_set += [inputs_and_outputs[i]]\n",
    "                    else:\n",
    "                        self._test_set += [inputs_and_outputs[i]]\n",
    "                else:\n",
    "                    self._test_set += [inputs_and_outputs[i]]\n",
    "\n",
    "        self.was_read = True\n",
    "\n",
    "        if self.stratify_by_pos:\n",
    "            # Make sure there is no overlap (or leakage) between the\n",
    "            # train/validate and test sets in the split; to do this, toss\n",
    "            # test data that is too \"close\" (according to the below function) to\n",
    "            # the train/validate data\n",
    "            train_and_validate = self._train_set + self._validate_set\n",
    "\n",
    "            test_set_nonoverlapping, _ = self.make_nonoverlapping_datasets(\n",
    "                    train_and_validate, self._test_set)\n",
    "            assert len(test_set_nonoverlapping) <= len(self._test_set)\n",
    "            self._test_set = test_set_nonoverlapping\n",
    "            \n",
    "#######################################################################################################\n",
    "            # The data points should still be shuffled; currently they\n",
    "            # are sorted within each data set by position in the target\n",
    "            random.shuffle(self._train_set)\n",
    "            random.shuffle(self._validate_set)\n",
    "            random.shuffle(self._test_set)\n",
    "\n",
    "        # Verify the correctness of self.pos_for_input(); it's key for later\n",
    "        # steps\n",
    "        # Include making a numpy array out of input_feats, as done when\n",
    "        # generating the data sets\n",
    "        for row in rows:\n",
    "            pos = int(row['guide_pos_nt'])\n",
    "            input_feats, _ = self._gen_input_and_output(row)\n",
    "            input_feats = np.array(input_feats, dtype='f')\n",
    "            assert self.pos_for_input(input_feats) == pos\n",
    "\n",
    "\n",
    "    def pos_for_input(self, x):\n",
    "        \"\"\"Return position (in nucleotide space) of the crRNA of a data point.\n",
    "        Args:\n",
    "            x: data point, namely input features\n",
    "        Returns:\n",
    "            x's position in nucleotide space\n",
    "        \"\"\"\n",
    "        if not self.was_read:\n",
    "            raise Exception(\"read() must be called first\")\n",
    "        x_key = np.array(x, dtype='f').tostring()\n",
    "        assert x_key in self.input_feats_pos\n",
    "        return self.input_feats_pos[x_key]\n",
    "\n",
    "    def unique_sampling_idx(self, num_unique_sample, xx):\n",
    "        \"\"\"Return indices for sampling unique data points.\n",
    "        The data contains replicate measurements. which complicates sampling\n",
    "        from it. If we wanted to subsample a small fraction of the data, it\n",
    "        would be possible that we end up sampling a large fraction or all of\n",
    "        the unique data points (with, on average, just a small fraction of\n",
    "        replicate measurements for each unique data point).\n",
    "        Args:\n",
    "            num_unique_sample: number of unique data points to sample\n",
    "            xx: collection of data points, including replicates\n",
    "        Returns:\n",
    "            list of indices in xx to sample, including all replicates for\n",
    "                each of num_unique_sample unique data points\n",
    "        \"\"\"\n",
    "        if not self.was_read:\n",
    "            raise Exception(\"read() must be called first\")\n",
    "\n",
    "        def key(x):\n",
    "            return np.array(x, dtype='f').tostring()\n",
    "\n",
    "        # Sample keys corresponding to unique data points (i.e.,\n",
    "        # as given by key(x))\n",
    "        xx_keys = set(key(x) for x in xx)\n",
    "        sampled_keys = set(random.sample(list(xx_keys), num_unique_sample))\n",
    "\n",
    "        # Determine all indices in xx that correspond to the unique\n",
    "        # data points sampled (with replicates)\n",
    "        idx = [i for i in range(len(xx)) if key(xx[i]) in sampled_keys]\n",
    "        return idx\n",
    "\n",
    "    def num_unique_points(self, xx):\n",
    "        \"\"\"Return number of unique data points.\n",
    "        The data contains replicate measurements, so this determines\n",
    "        how many represent unique guide-target pairs.\n",
    "        Args:\n",
    "            xx: collection of data points, including replicates\n",
    "        Returns:\n",
    "            number of unique points in xx\n",
    "        \"\"\"\n",
    "        if not self.was_read:\n",
    "            raise Exception(\"read() must be called first\")\n",
    "\n",
    "        def key(x):\n",
    "            return np.array(x, dtype='f').tostring()\n",
    "\n",
    "        return len(set(key(x) for x in xx))\n",
    "    \n",
    "#######################################################################################################################\n",
    "    def make_nonoverlapping_datasets(self, data1, data2):\n",
    "        \"\"\"Make sure there is no overlap (leakage) between two datasets.\n",
    "        Each data point takes up some region in nucleotide space. This\n",
    "        makes sure that the crRNAs from two datasets do not overlap in that\n",
    "        nucleotide space -- i.e., there is not leakage between the two.\n",
    "        To do this, this modifies data2 by removing points that overlap\n",
    "        in nucleotide space with points in data1.\n",
    "        Note that this permits sequence context of points in data2 to overlap\n",
    "        with crRNAs for points in data1, and vice-versa. Ensuring no overlap\n",
    "        of sequence context would probably require tossing too many data\n",
    "        points, if self.context_nt is large.\n",
    "        Args:\n",
    "            data1: list of tuples (X, y) where X is input data and y is outputs\n",
    "            data2: list of tuples (X, y) where X is input data and y is outputs\n",
    "        Returns:\n",
    "            tuple (a, b) where a is data2, after having removed any data\n",
    "            points in data2 that overlap with ones in data1, and b is\n",
    "            the indices in data2 that were kept\n",
    "        \"\"\"\n",
    "        def range_for_input(x):\n",
    "            # Compute x's crRNA range in nucleotide space: (start, end) where\n",
    "            # start is inclusive and end is exclusive\n",
    "            start_pos_nt = self.pos_for_input(x)\n",
    "            length_in_nt = self.CRRNA_LEN\n",
    "            # print('start_post_nt:',start_pos_nt)\n",
    "            # print('length_in_nt:',length_in_nt)\n",
    "            return (start_pos_nt, start_pos_nt + length_in_nt)\n",
    "\n",
    "        # The nucleotide space in this dataset is small (~1000 nt); just\n",
    "        # create a set of all nucleotides in data1 and check data2 against\n",
    "        # this\n",
    "        # Since many ranges will be the same (i.e., points are at the exact\n",
    "        # same position), first create a set of ranges and then create\n",
    "        # a set of nucleotide positions from those\n",
    "        data1_ranges = set()\n",
    "        for X, y in data1:\n",
    "            X_start, X_end = range_for_input(X)\n",
    "            data1_ranges.add((X_start, X_end))\n",
    "        data1_nt = set()\n",
    "        for start, end in data1_ranges:\n",
    "            for p in range(start, end):\n",
    "                data1_nt.add(p)\n",
    "\n",
    "        # Find which points in data2 to remove because they overlap a\n",
    "        # nucleotide in data1\n",
    "        # Make a copy of data2, only including points that do not overlap\n",
    "        data2_nonoverlapping = []\n",
    "        data2_idx_kept = []\n",
    "        for i, (X, y) in enumerate(data2):\n",
    "            X_start, X_end = range_for_input(X)\n",
    "            include = True\n",
    "            for p in range(X_start, X_end):\n",
    "                if p in data1_nt:\n",
    "                    # Remove this data point from data2\n",
    "                    include = False\n",
    "                    break\n",
    "            if include:\n",
    "                data2_nonoverlapping += [(X, y)]\n",
    "                data2_idx_kept += [i]\n",
    "\n",
    "        return (data2_nonoverlapping, data2_idx_kept)\n",
    "\n",
    "    def _data_set(self, data):\n",
    "        \"\"\"Return data set.\n",
    "        Args:\n",
    "            data: one of self._train_set, self._validate_set, or\n",
    "                self._test_set\n",
    "        Returns:\n",
    "            (X, y) where X is input vectors and y is outputs\n",
    "        \"\"\"\n",
    "        if not self.was_read:\n",
    "            raise Exception(\"read() must be called first\")\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        for input_feats, output in data:\n",
    "            inputs += [input_feats]\n",
    "            outputs += [[output]]\n",
    "        return np.array(inputs, dtype='f'), np.array(outputs, dtype='f')\n",
    "\n",
    "    def train_set(self):\n",
    "        \"\"\"Return training set.\n",
    "        Returns:\n",
    "            (X, y) where X is input vectors and y is outputs\n",
    "        \"\"\"\n",
    "        return self._data_set(self._train_set)\n",
    "\n",
    "    def validate_set(self):\n",
    "        \"\"\"Return validation set.\n",
    "        Returns:\n",
    "            (X, y) where X is input vectors and y is outputs\n",
    "        \"\"\"\n",
    "        return self._data_set(self._validate_set)\n",
    "\n",
    "    def test_set(self):\n",
    "        \"\"\"Return test set.\n",
    "        Returns:\n",
    "            (X, y) where X is input vectors and y is outputs\n",
    "        \"\"\"\n",
    "        return self._data_set(self._test_set)\n",
    "\n",
    "    def sample_regression_weight(self, xi, yi, p=0):\n",
    "        \"\"\"Compute a sample weight to use during regression while training.\n",
    "        Args:\n",
    "            xi: data point, namely input features\n",
    "            yi: activity of xi\n",
    "            p: scaling factor for importance weight (p>=0; p=0 does not incorporate\n",
    "                sample importance, and all samples will have the same weight)\n",
    "        Returns:\n",
    "            relative weight of sample\n",
    "        \"\"\"\n",
    "        # xi is a guide-target pair\n",
    "        # Determine the mean and stdev across all targets of the guide in xi\n",
    "        guide_pos = self.pos_for_input(xi)\n",
    "        guide_wildtype_mean = self.crrna_activity_mean[guide_pos]\n",
    "        guide_wildtype_stdev = self.crrna_activity_stdev[guide_pos]\n",
    "\n",
    "        # Let the weight be 1 + p*|z|, where z is\n",
    "        #     ([guide activity for xi] - [mean activity across targets at xi]) /\n",
    "        #         [stdev of activity across targets at xi]\n",
    "        # This way guide-target pairs where the activity is much more different\n",
    "        # than the average for the guide are weighted more heavily during\n",
    "        # training; intuitively, these are more interesting/important samples so we\n",
    "        # want to weight them higher, and also the variation within guides (i.e.,\n",
    "        # for target variants of a guide) may be harder to learn than the variation\n",
    "        # across guides\n",
    "        z = (yi - guide_wildtype_mean) / guide_wildtype_stdev\n",
    "        weight = 1 + p*np.absolute(z)\n",
    "\n",
    "        return weight\n",
    "\n",
    "    def split(self, x, y, num_splits, shuffle_and_split=False,\n",
    "            stratify_by_pos=False, yield_indices=False):\n",
    "        \"\"\"Split the data using stratified folds, for k-fold cross validation.\n",
    "        Args:\n",
    "            x: input data\n",
    "            y: labels\n",
    "            num_splits: number of folds\n",
    "            shuffle_and_split: if True, shuffle before splitting and stratify based\n",
    "                on the distribution of output variables (here, classes) to ensure they\n",
    "                are roughly the same across the different folds\n",
    "            stratify_by_pos: if True, determine the different folds based on\n",
    "                the position of each data point (ensuring that the\n",
    "                validate set is a contiguous region of the target molecule; this is\n",
    "                similar to leave-one-gene-out cross-validation\n",
    "            yield_indices: if True, return indices rather than data points (see\n",
    "                'Iterates:' below for detail)\n",
    "        Iterates:\n",
    "            if yield_indices is False:\n",
    "                (x_train_i, y_train_i, x_validate_i, y_validate_i) where each is\n",
    "                for a fold of the data; these are actual data points from x, y\n",
    "            if yield_indices is True:\n",
    "                (train_i, validate_i) where each is for a fold of the data;\n",
    "                these are indices referring to data in x, y\n",
    "        \"\"\"\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        if ((shuffle_and_split is False and stratify_by_pos is False) or\n",
    "                (shuffle_and_split is True and stratify_by_pos is True)):\n",
    "            raise ValueError((\"Exactly one of shuffle_and_split or stratify_by_pos \"\n",
    "                \"must be set\"))\n",
    "\n",
    "        if shuffle_and_split:\n",
    "            idx = list(range(len(x)))\n",
    "            random.shuffle(idx)\n",
    "            x_shuffled = [x[i] for i in idx]\n",
    "            y_shuffled = [y[i] for i in idx]\n",
    "            x = np.array(x_shuffled)\n",
    "            y = np.array(y_shuffled)\n",
    "            skf = StratifiedKFold(n_splits=num_splits)\n",
    "            def split_iter():\n",
    "                return skf.split(x, y)\n",
    "        elif stratify_by_pos:\n",
    "            x_with_pos = [(xi, self.pos_for_input(xi)) for xi in x]\n",
    "            all_pos = np.array(sorted(set(pos for xi, pos in x_with_pos)))\n",
    "            kf = KFold(n_splits=num_splits)\n",
    "            def split_iter():\n",
    "                for train_pos_idx, test_pos_idx in kf.split(all_pos):\n",
    "                    train_pos = set(all_pos[train_pos_idx])\n",
    "                    test_pos = set(all_pos[test_pos_idx])\n",
    "                    train_index, test_index = [], []\n",
    "                    for i, (xi, pos) in enumerate(x_with_pos):\n",
    "                        assert pos in train_pos or pos in test_pos\n",
    "                        if pos in train_pos:\n",
    "                            train_index += [i]\n",
    "                        if pos in test_pos:\n",
    "                            test_index += [i]\n",
    "\n",
    "                    # Get rid of test indices for data points that overlap with\n",
    "                    # ones in the train set\n",
    "                    x_train = [(x[i], y[i]) for i in train_index]\n",
    "                    x_test = [(x[i], y[i]) for i in test_index]\n",
    "                    _, test_index_idx_to_keep = self.make_nonoverlapping_datasets(\n",
    "                            x_train, x_test)\n",
    "                    test_index_idx_to_keep = set(test_index_idx_to_keep)\n",
    "                    test_index_nonoverlapping = []\n",
    "                    for i in range(len(test_index)):\n",
    "                        if i in test_index_idx_to_keep:\n",
    "                            test_index_nonoverlapping += [test_index[i]]\n",
    "                    test_index = test_index_nonoverlapping\n",
    "\n",
    "                    # Shuffle order of data points within the train and test sets\n",
    "                    random.shuffle(train_index)\n",
    "                    random.shuffle(test_index)\n",
    "                    yield train_index, test_index\n",
    "\n",
    "        for train_index, test_index in split_iter():\n",
    "            if yield_indices:\n",
    "                yield (train_index, test_index)\n",
    "            else:\n",
    "                x_train_i, y_train_i = x[train_index], y[train_index]\n",
    "                x_validate_i, y_validate_i = x[test_index], y[test_index]\n",
    "                yield (x_train_i, y_train_i, x_validate_i, y_validate_i)\n",
    "\n",
    "    def seq_features_from_encoding(self, x):\n",
    "        \"\"\"Determine sequence features by parsing the input vector for a data point.\n",
    "        In some ways this reverses the parsing done above. This converts a one-hot\n",
    "        encoding of both target and guide back into nucleotide sequence space.\n",
    "        Args:\n",
    "            x: input sequence as Lx8 vector where L is the target length; x[i][0:4]\n",
    "                gives a one-hot encoding of the target at position i and\n",
    "                x[i][4:8] gives a one-hot encoding ofthe guide at position i\n",
    "        Returns:\n",
    "            dict where keys are features (e.g., 'target', 'guide', and 'PFS')\n",
    "        \"\"\"\n",
    "        x = np.array(x)\n",
    "\n",
    "        target_len = len(x)\n",
    "        assert x.shape == (target_len, 8)\n",
    "\n",
    "        # Read the target\n",
    "        target = ''.join(oh_to_nt(x[i][0:4]) for i in range(target_len))\n",
    "\n",
    "        # Everything in the target should be a nucleotide\n",
    "        assert '-' not in target\n",
    "\n",
    "        # Read the guide\n",
    "        guide_with_context = ''.join(oh_to_nt(x[i][4:8]) for i in range(target_len))\n",
    "\n",
    "        # Verify the context of the guide is all '-' (all 0s), and extract just\n",
    "        # the guide\n",
    "        guide_start = self.context_nt\n",
    "        guide_end = target_len - self.context_nt\n",
    "        assert guide_with_context[:guide_start] == '-'*self.context_nt\n",
    "        assert guide_with_context[guide_end:] == '-'*self.context_nt\n",
    "        guide = guide_with_context[guide_start:guide_end]\n",
    "        assert '-' not in guide\n",
    "        target_without_context = target[guide_start:guide_end]\n",
    "\n",
    "        # Compute the Hamming distance\n",
    "        hd = sum(1 for i in range(len(guide)) if guide[i] != target_without_context[i])\n",
    "\n",
    "        # Determine the Cas13a PFS (immediately adjacent to guide,\n",
    "        # 3' end in target space)\n",
    "        cas13a_pfs = target[guide_end]\n",
    "\n",
    "        return {'target': target,\n",
    "                'target_without_context': target_without_context,\n",
    "                'guide': guide,\n",
    "                'hamming_dist': hd,\n",
    "                'cas13a_pfs': cas13a_pfs}\n",
    "\n",
    "\n",
    "def input_vec_for_embedding(x, context_nt):\n",
    "    \"\"\"Create an input vector to use with an embedding layer, from one-hot\n",
    "    encoded sequence.\n",
    "    Args:\n",
    "        x: input sequence as Lx8 vector where L is the target length; x[i][0:4]\n",
    "            gives a one-hot encoding of the target at position i and\n",
    "            x[i][4:8] gives a one-hot encoding ofthe guide at position i\n",
    "        context_nt: amount of context in target\n",
    "    Returns:\n",
    "        Lx2 vector where x[i][0] gives an index corresponding to a nucleotide\n",
    "        of the target at position i and x[i][1] for the guide. Each x[i][j]\n",
    "        is an index in [0,4] -- 0 for A, 1 for C, 2 for G, 3 for T, and 4\n",
    "        for no guide sequence (i.e., target context)\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "\n",
    "    target_len = len(x)\n",
    "    assert x.shape == (target_len, 8)\n",
    "\n",
    "    # Read the target\n",
    "    target = ''.join(oh_to_nt(x[i][0:4]) for i in range(target_len))\n",
    "\n",
    "    # Everything in the target should be a nucleotide\n",
    "    assert '-' not in target\n",
    "\n",
    "    # Read the guide\n",
    "    guide_with_context = ''.join(oh_to_nt(x[i][4:8]) for i in range(target_len))\n",
    "\n",
    "    # Verify the context of the guide is all '-' (all 0s)\n",
    "    guide_start = context_nt\n",
    "    guide_end = target_len - context_nt\n",
    "    assert guide_with_context[:guide_start] == '-'*context_nt\n",
    "    assert guide_with_context[guide_end:] == '-'*context_nt\n",
    "\n",
    "    idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3, '-': 4}\n",
    "    x_idx = [[idx[target[i]], idx[guide_with_context[i]]]\n",
    "                for i in range(target_len)]\n",
    "    return np.array(x_idx)\n",
    "\n",
    "\n",
    "\n",
    "class CasCNNWithParallelFilters(tf.keras.Model):\n",
    "    def __init__(self, params, regression):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            params: dict of hyperparameters\n",
    "            regression: if True, perform regression; if False, classification\n",
    "        \"\"\"\n",
    "        super(CasCNNWithParallelFilters, self).__init__()\n",
    "\n",
    "        self.regression = regression\n",
    "        self.add_gc_content = params['add_gc_content']\n",
    "        self.context_nt = params['context_nt']\n",
    "\n",
    "        # Note that this is only used for regression\n",
    "        if params['sample_weight_scaling_factor'] < 0:\n",
    "            raise ValueError((\"Parameter 'sample_weight_scaling_factor' \"\n",
    "                \"must be >=0\"))\n",
    "        self.sample_weight_scaling_factor = params['sample_weight_scaling_factor']\n",
    "\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "\n",
    "        if self.add_gc_content:\n",
    "            # Construct a layer to extract the region (along the width axis)\n",
    "            # of the guide sequence (and target)\n",
    "            self.guide_slice = tf.keras.layers.Cropping1D((self.context_nt,\n",
    "                self.context_nt))\n",
    "\n",
    "        if params['conv_filter_width'] is None:\n",
    "            # Use a layer of width 'None', to make it easier to construct\n",
    "            # locally connected layers below\n",
    "            conv_filter_widths = [None]\n",
    "        else:\n",
    "            conv_filter_widths = params['conv_filter_width']\n",
    "\n",
    "        # Construct groups, where each consists of a convolutional\n",
    "        # layer with a particular width, a batch normalization layer, and\n",
    "        # a pooling layer\n",
    "        # Store these in separate lists, rather than as tuples in a single\n",
    "        # list, so that they get stored in self.layers\n",
    "        self.convs = []\n",
    "        self.batchnorms = []\n",
    "        self.pools = []\n",
    "        self.pools_2 = []\n",
    "        self.lcs = []\n",
    "        for filter_width in conv_filter_widths:\n",
    "            if filter_width is not None:\n",
    "                # Construct the convolutional layer\n",
    "                # Do not pad the input (`padding='valid'`) because all input\n",
    "                # sequences should be the same length\n",
    "                conv_layer_num_filters = params['conv_num_filters'] # ie, num of output channels\n",
    "                conv = tf.keras.layers.Conv1D(\n",
    "                        conv_layer_num_filters,\n",
    "                        filter_width,\n",
    "                        strides=1,  # stride by 1\n",
    "                        padding='valid',\n",
    "                        activation=params['activation_fn'],\n",
    "                        name='group_w' + str(filter_width) + '_conv')\n",
    "                # Note that the total number of filters in this layer will be\n",
    "                # len(conv_filter_width)*conv_layer_num_filters since there are\n",
    "                # len(conv_filter_width) groups\n",
    "\n",
    "                # Add a batch normalization layer\n",
    "                # It should not matter whether this comes before or after the\n",
    "                # pool layer, as long as it is after the conv layer\n",
    "                # This is applied after the activation of the conv layer; the\n",
    "                # original batch norm applies batch normalization before the\n",
    "                # activation function, but more recent work seems to apply it\n",
    "                # after activation\n",
    "                # Only use if the parameter specifying to skip batch norm is\n",
    "                # not set\n",
    "                if params['skip_batch_norm'] is True:\n",
    "                    batchnorm = None\n",
    "                else:\n",
    "                    batchnorm = tf.keras.layers.BatchNormalization(\n",
    "                            name='group_w' + str(filter_width) + '_batchnorm')\n",
    "\n",
    "                # Add a pooling layer\n",
    "                # Pool over a window of width pool_window, for\n",
    "                # each output channel of the conv layer (and, of course, for each batch)\n",
    "                # Stride by pool_stride; note that if  pool_stride = pool_window,\n",
    "                # then the pooling windows are non-overlapping\n",
    "                pool_window_width = params['pool_window_width']\n",
    "                pool_stride = max(1, int(pool_window_width / 2))\n",
    "                maxpool = tf.keras.layers.MaxPooling1D(\n",
    "                        pool_size=pool_window_width,\n",
    "                        strides=pool_stride,\n",
    "                        name='group_w' + str(filter_width) + '_maxpool')\n",
    "                avgpool = tf.keras.layers.AveragePooling1D(\n",
    "                        pool_size=pool_window_width,\n",
    "                        strides=pool_stride,\n",
    "                        name='group_w' + str(filter_width) + '_avgpool')\n",
    "\n",
    "                self.convs += [conv]\n",
    "                self.batchnorms += [batchnorm]\n",
    "\n",
    "                # If only using 1 pool, store this in self.pools\n",
    "                # If using 2 pools, store one in self.pools and the other in\n",
    "                # self.pools_2, and create self.pool_merge to concatenate the\n",
    "                # outputs of these 2 pools\n",
    "                if params['pool_strategy'] == 'max':\n",
    "                    self.pools += [maxpool]\n",
    "                    self.pools_2 += [None]\n",
    "                elif params['pool_strategy'] == 'avg':\n",
    "                    self.pools += [avgpool]\n",
    "                    self.pools_2 += [None]\n",
    "                elif params['pool_strategy'] == 'max-and-avg':\n",
    "                    self.pools += [maxpool]\n",
    "                    self.pools_2 += [avgpool]\n",
    "                else:\n",
    "                    raise Exception((\"Unknown --pool-strategy\"))\n",
    "            else:\n",
    "                # No convolutional layer\n",
    "                self.convs += [None]\n",
    "                self.batchnorms += [None]\n",
    "                self.pools += [None]\n",
    "                self.pools_2 += [None]\n",
    "\n",
    "            # Setup locally connected layers (if set)\n",
    "            # Use one for each convolution filter grouping (if applied after\n",
    "            # concatenating the groups, then the position-dependence may be\n",
    "            # less meaningful because a single locally connected neuron may\n",
    "            # be connected across two different groups)\n",
    "            # This layer can be useful in this application because (unlike\n",
    "            # convolution layers) it explicitly models the position-dependence --\n",
    "            # i.e., weights can differ across a guide and across the sequence\n",
    "            # context. Moreover, it can help collapse the different convolutional\n",
    "            # filters down to a smaller number of values (dimensions) at\n",
    "            # each position, effectively serving as a dimensionality reduction\n",
    "            # before the fully connected layers.\n",
    "            if params['locally_connected_width'] is not None:\n",
    "                lcs_for_conv = []\n",
    "                locally_connected_dim = params['locally_connected_dim']\n",
    "                for i, lc_width in enumerate(params['locally_connected_width']):\n",
    "                    if filter_width is not None:\n",
    "                        name = 'group_w' + str(filter_width) + '_lc_w' + str(lc_width)\n",
    "                    else:\n",
    "                        name = 'lc_w' + str(lc_width)\n",
    "                    # Stride by 1/2 the width\n",
    "                    stride = max(1, int(lc_width / 2))\n",
    "                    lc = tf.keras.layers.LocallyConnected1D(\n",
    "                        locally_connected_dim,\n",
    "                        lc_width,\n",
    "                        strides=stride,\n",
    "                        activation=params['activation_fn'],\n",
    "                        name=name)\n",
    "                    lcs_for_conv += [lc]\n",
    "                self.lcs += [lcs_for_conv]\n",
    "            else:\n",
    "                self.lcs += [None]\n",
    "\n",
    "        if conv_filter_widths != [None] and params['pool_strategy'] == 'max-and-avg':\n",
    "            # Setup layer to concatenate each max/avg pooling in each group\n",
    "            self.pool_merge = tf.keras.layers.Concatenate(\n",
    "                    axis=1,\n",
    "                    name='merge_pool')\n",
    "\n",
    "        if params['locally_connected_width'] is not None:\n",
    "            if len(params['locally_connected_width']) > 1:\n",
    "                # Setup layer to concatenate the locally connected layers\n",
    "                # in each group\n",
    "                self.lc_merge = tf.keras.layers.Concatenate(\n",
    "                        axis=1,\n",
    "                        name='merge_lc')\n",
    "\n",
    "        # Merge the outputs of the groups\n",
    "        # The concatenation needs to happen along an axis, and all\n",
    "        # inputs must have the same dimension along each axis except\n",
    "        # for the concat axis\n",
    "        # The axes are: (batch size, width, filters)\n",
    "        # The concatenation can happen along either the width axis (1)\n",
    "        # or the filters axis (2); it should not make a difference\n",
    "        # Because the filters axis will all be the same dimension\n",
    "        # (conv_layer_num_filters or, if there are locally connected layers,\n",
    "        # then locally_connected_dim) but the width axis may be slightly\n",
    "        # different (as each filter/kernel has a different width, so\n",
    "        # the number that span the input may differ slightly), let's\n",
    "        # concatenate along the width axis (axis=1)\n",
    "        # Only create the merge layer if it is needed (i.e., there are\n",
    "        # multiple filter widths)\n",
    "        if len(conv_filter_widths) > 1:\n",
    "            self.merge = tf.keras.layers.Concatenate(\n",
    "                    axis=1,\n",
    "                    name='merge_groups')\n",
    "\n",
    "        # Flatten the pooling output from above while preserving\n",
    "        # the batch axis\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        # Setup fully connected layers\n",
    "        # Insert dropout before each of them for regularization\n",
    "        # Set the dimension of each fully connected layer (i.e., dimension\n",
    "        # of the output space) to params['fully_connected_dim'][i]\n",
    "        self.dropouts = []\n",
    "        self.fcs = []\n",
    "        for i, fc_hidden_dim in enumerate(params['fully_connected_dim']):\n",
    "            dropout = tf.keras.layers.Dropout(\n",
    "                    params['dropout_rate'],\n",
    "                    name='dropout_' + str(i+1))\n",
    "            fc = tf.keras.layers.Dense(\n",
    "                    fc_hidden_dim,\n",
    "                    activation=params['activation_fn'],\n",
    "                    name='fc_' + str(i+1))\n",
    "            self.dropouts += [dropout]\n",
    "            self.fcs += [fc]\n",
    "\n",
    "        # Construct the final layer (fully connected)\n",
    "        fc_final_dim = 1\n",
    "        if regression:\n",
    "            final_activation = 'linear'\n",
    "        else:\n",
    "            final_activation = 'sigmoid'\n",
    "        self.fc_final = tf.keras.layers.Dense(\n",
    "                fc_final_dim,\n",
    "                activation=final_activation,\n",
    "                name='fc_final')\n",
    "\n",
    "        if (regression and 'regression_clip' in params and\n",
    "                params['regression_clip'] is True):\n",
    "            # Clip output to be >= min_out\n",
    "            min_out = -4\n",
    "            def clip(x):\n",
    "                return tf.keras.activations.relu(x - min_out) + min_out\n",
    "            self.clip_output = clip\n",
    "            alpha = params['regression_clip_alpha']\n",
    "            def clip_leaky(x):\n",
    "                return tf.keras.activations.relu(x - min_out,\n",
    "                        alpha=alpha) + min_out\n",
    "            self.clip_output_leaky = clip_leaky\n",
    "        else:\n",
    "            self.clip_output = None\n",
    "\n",
    "        # Regularize weights on each layer\n",
    "        l2_regularizer = tf.keras.regularizers.l2(params['l2_factor'])\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'kernel_regularizer'):\n",
    "                layer.kernel_regularizer = l2_regularizer\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Run parallel convolution filters of different widths, each with\n",
    "        # batch norm and pooling\n",
    "        # If set, also add a locally connected layer(s) for each group\n",
    "\n",
    "        # If self.add_gc_content is True, then 'manually' add a feature to\n",
    "        # the fully connected layer giving the GC content of the guide\n",
    "        if self.add_gc_content:\n",
    "            # Extract the region of the guide/target\n",
    "            x_guide_region = self.guide_slice(x)\n",
    "            # Pull out just the guide; in dimension 3, the first 4 bases\n",
    "            # are target and the next 4 are guide\n",
    "            x_guide = tf.slice(x_guide_region,\n",
    "                    [0, 0, 4],  # begin at position 4 in dimension 3\n",
    "                    [-1, -1, 4])    # take the 4 positions of the guide\n",
    "            # Compute the number of bases for each base by summing the\n",
    "            # one-hot encoding for the corresponding dimension\n",
    "            base_count = tf.reduce_sum(x_guide, axis=1, keepdims=True)\n",
    "            # Pull out just the C and G counts, and sum across these\n",
    "            gc_count = base_count[:,:,1] + base_count[:,:,2]\n",
    "            guide_len = tf.cast(tf.shape(x_guide)[1], tf.float32)\n",
    "            gc_content = gc_count / guide_len\n",
    "\n",
    "        group_outputs = []\n",
    "        for conv, batchnorm, pool_1, pool_2, lcs in zip(self.convs, self.batchnorms,\n",
    "                self.pools, self.pools_2, self.lcs):\n",
    "            if conv is not None:\n",
    "                # Run the convolutional layer and batch norm on x, to\n",
    "                # start this group\n",
    "                group_x = conv(x)\n",
    "                if batchnorm is not None:\n",
    "                    group_x = batchnorm(group_x, training=training)\n",
    "\n",
    "                # Run the pooling layer on the current group output (group_x)\n",
    "                if pool_2 is None:\n",
    "                    group_x = pool_1(group_x)\n",
    "                else:\n",
    "                    group_x_1 = pool_1(group_x)\n",
    "                    group_x_2 = pool_2(group_x)\n",
    "                    group_x = self.pool_merge([group_x_1, group_x_2])\n",
    "            else:\n",
    "                # Skip the convolutional layer, as well as batch norm and\n",
    "                # pooling\n",
    "                group_x = x\n",
    "\n",
    "            if lcs is not None:\n",
    "                # Run the locally connected layer (1 or more)\n",
    "                if len(lcs) == 1:\n",
    "                    # Only 1 locally connected layer\n",
    "                    lc = lcs[0]\n",
    "                    group_x = lc(group_x)\n",
    "                else:\n",
    "                    lc_outputs = []\n",
    "                    for lc in lcs:\n",
    "                        # Run the locally connected layer (lc) on the\n",
    "                        # current output for this group (group_x)\n",
    "                        lc_outputs += [lc(group_x)]\n",
    "                    # Merge the outputs of the locally connected layers\n",
    "                    group_x = self.lc_merge(lc_outputs)\n",
    "\n",
    "            group_outputs += [group_x]\n",
    "\n",
    "        # Merge the above groups\n",
    "        if len(group_outputs) == 1:\n",
    "            # Only 1 filter width; cannot merge across 1 input\n",
    "            x = group_outputs[0]\n",
    "        else:\n",
    "            x = self.merge(group_outputs)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        if self.add_gc_content:\n",
    "            # Concatenate x and gc_content along the flattened dimension\n",
    "            # (axis 0 is batch; use '-1' or '1' for the axis to concat along)\n",
    "            x_with_gc = tf.concat([x, gc_content], -1)\n",
    "            x = x_with_gc\n",
    "\n",
    "        # Run through fully connected layers\n",
    "        for dropout, fc in zip(self.dropouts, self.fcs):\n",
    "            x = dropout(x, training=training)\n",
    "            x = fc(x)\n",
    "\n",
    "        x = dropout(x, training=training)\n",
    "        x = self.fc_final(x)\n",
    "        if self.clip_output is not None:\n",
    "            if training:\n",
    "                x = self.clip_output_leaky(x)\n",
    "            else:\n",
    "                x = self.clip_output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def construct_model(params, shape, regression=True):\n",
    "    \"\"\"Construct model.\n",
    "    Args:\n",
    "        params: dict of hyperparameters\n",
    "        shape: shape of input data; only used for printing model summary\n",
    "        regression: if True, perform regression; if False, classification\n",
    "    Returns:\n",
    "        CasCNNWithParallelFilters object\n",
    "    \"\"\"\n",
    "    model = CasCNNWithParallelFilters(params, regression)\n",
    "\n",
    "    # Print a model summary\n",
    "    model.build(shape)\n",
    "    # print(model.summary())\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8faba67-bd75-49dd-ac56-e1bf3a84f8c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {'dataset':'cas13',\n",
    "          'cas13_subset':'exp',\n",
    "          'cas13-regress-on-all': True,\n",
    "          'cas13-regress-only-on-active':False,\n",
    "          'context_nt':10,\n",
    "          'conv_filter_width':[1,2],\n",
    "          'conv_num_filters':25,\n",
    "          'pool_window_width':2,\n",
    "          'fully_connected_dim':[53],\n",
    "          'pool_strategy':'max',\n",
    "          'locally_connected_width':[1,2],\n",
    "          'locally_connected_dim':3,\n",
    "          'skip_batch_norm': True,\n",
    "          'add_gc_content':True,\n",
    "          'activation_fn':'relu',\n",
    "          'dropout_rate':0.25,\n",
    "          'l2_factor':0,\n",
    "          'sample_weight_scaling_factor':0,\n",
    "          'batch_size':32,\n",
    "          'learning_rate':0.00001,\n",
    "          'max_num_epochs':1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51d93bc6-c94d-46ce-a2e4-f233fa4591ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt = construct_model(params=params,shape=(None,48,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ae0bf73-8a3e-4f67-87e4-6b3aaf4c732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/n_q459w55tq87t1kfxnp8hrr0000gn/T/ipykernel_15634/4262826452.py:448: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  input_feats_key = np.array(input_feats, dtype='f').tostring()\n",
      "/var/folders/8c/n_q459w55tq87t1kfxnp8hrr0000gn/T/ipykernel_15634/4262826452.py:525: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  x_key = np.array(x, dtype='f').tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15300, 68, 8)\n",
      "(3824, 68, 8)\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 18:32:24.680015: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/479 [==============================] - ETA: 0s - loss: 3.0048"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 18:32:47.971539: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/479 [==============================] - 27s 36ms/step - loss: 3.0048 - val_loss: 2.3643\n",
      "Epoch 2/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.4009 - val_loss: 1.5737\n",
      "Epoch 3/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.3165 - val_loss: 1.5435\n",
      "Epoch 4/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.2951 - val_loss: 1.5278\n",
      "Epoch 5/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.2707 - val_loss: 1.5199\n",
      "Epoch 6/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 1.2444 - val_loss: 1.5300\n",
      "Epoch 7/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 1.2158 - val_loss: 1.4883\n",
      "Epoch 8/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 1.2025 - val_loss: 1.5011\n",
      "Epoch 9/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 1.1805 - val_loss: 1.5036\n",
      "Epoch 10/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.1524 - val_loss: 1.5205\n",
      "Epoch 11/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.1338 - val_loss: 1.4696\n",
      "Epoch 12/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 1.1183 - val_loss: 1.4837\n",
      "Epoch 13/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 1.0984 - val_loss: 1.5029\n",
      "Epoch 14/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.0803 - val_loss: 1.4614\n",
      "Epoch 15/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.0648 - val_loss: 1.4528\n",
      "Epoch 16/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.0448 - val_loss: 1.4576\n",
      "Epoch 17/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.0321 - val_loss: 1.4381\n",
      "Epoch 18/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 1.0128 - val_loss: 1.4327\n",
      "Epoch 19/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.9967 - val_loss: 1.4241\n",
      "Epoch 20/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.9783 - val_loss: 1.4693\n",
      "Epoch 21/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.9624 - val_loss: 1.4171\n",
      "Epoch 22/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.9447 - val_loss: 1.4387\n",
      "Epoch 23/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.9263 - val_loss: 1.4296\n",
      "Epoch 24/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.9118 - val_loss: 1.4102\n",
      "Epoch 25/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.8947 - val_loss: 1.4005\n",
      "Epoch 26/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.8740 - val_loss: 1.3733\n",
      "Epoch 27/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.8582 - val_loss: 1.3940\n",
      "Epoch 28/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.8375 - val_loss: 1.3687\n",
      "Epoch 29/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.8219 - val_loss: 1.3601\n",
      "Epoch 30/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.8025 - val_loss: 1.3807\n",
      "Epoch 31/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7872 - val_loss: 1.3671\n",
      "Epoch 32/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7668 - val_loss: 1.3203\n",
      "Epoch 33/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7507 - val_loss: 1.3052\n",
      "Epoch 34/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.7364 - val_loss: 1.2875\n",
      "Epoch 35/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7167 - val_loss: 1.2793\n",
      "Epoch 36/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7048 - val_loss: 1.2763\n",
      "Epoch 37/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.6904 - val_loss: 1.2694\n",
      "Epoch 38/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.6749 - val_loss: 1.2154\n",
      "Epoch 39/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.6611 - val_loss: 1.2171\n",
      "Epoch 40/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.6490 - val_loss: 1.2099\n",
      "Epoch 41/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.6376 - val_loss: 1.1988\n",
      "Epoch 42/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.6287 - val_loss: 1.1464\n",
      "Epoch 43/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.6133 - val_loss: 1.1727\n",
      "Epoch 44/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.6055 - val_loss: 1.1510\n",
      "Epoch 45/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5989 - val_loss: 1.1484\n",
      "Epoch 46/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5904 - val_loss: 1.1890\n",
      "Epoch 47/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5804 - val_loss: 1.1034\n",
      "Epoch 48/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5719 - val_loss: 1.0966\n",
      "Epoch 49/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5672 - val_loss: 1.1039\n",
      "Epoch 50/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5576 - val_loss: 1.0361\n",
      "Epoch 51/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5537 - val_loss: 1.0501\n",
      "Epoch 52/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5476 - val_loss: 1.0589\n",
      "Epoch 53/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5404 - val_loss: 1.0006\n",
      "Epoch 54/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5384 - val_loss: 1.0099\n",
      "Epoch 55/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5336 - val_loss: 1.0317\n",
      "Epoch 56/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5297 - val_loss: 1.0622\n",
      "Epoch 57/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5267 - val_loss: 1.0170\n",
      "Epoch 58/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5209 - val_loss: 0.9803\n",
      "Epoch 59/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5195 - val_loss: 1.0043\n",
      "Epoch 60/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5132 - val_loss: 0.9738\n",
      "Epoch 61/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5103 - val_loss: 1.0133\n",
      "Epoch 62/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5080 - val_loss: 0.9674\n",
      "Epoch 63/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5053 - val_loss: 0.9498\n",
      "Epoch 64/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5009 - val_loss: 0.9343\n",
      "Epoch 65/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5035 - val_loss: 0.9532\n",
      "Epoch 66/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4957 - val_loss: 0.9326\n",
      "Epoch 67/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.4949 - val_loss: 0.9495\n",
      "Epoch 68/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4938 - val_loss: 0.9139\n",
      "Epoch 69/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4900 - val_loss: 0.9263\n",
      "Epoch 70/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4884 - val_loss: 0.9308\n",
      "Epoch 71/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4899 - val_loss: 0.8590\n",
      "Epoch 72/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4874 - val_loss: 0.9346\n",
      "Epoch 73/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4856 - val_loss: 0.8850\n",
      "Epoch 74/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4805 - val_loss: 0.9359\n",
      "Epoch 75/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4829 - val_loss: 0.8905\n",
      "Epoch 76/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4808 - val_loss: 0.8632\n",
      "Epoch 77/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4821 - val_loss: 0.9099\n",
      "Epoch 78/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4768 - val_loss: 0.8813\n",
      "Epoch 79/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4732 - val_loss: 0.8963\n",
      "Epoch 80/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4751 - val_loss: 0.8846\n",
      "Epoch 81/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4723 - val_loss: 0.8603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 18:50:24.076793: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/n_q459w55tq87t1kfxnp8hrr0000gn/T/ipykernel_15634/4262826452.py:448: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  input_feats_key = np.array(input_feats, dtype='f').tostring()\n",
      "/var/folders/8c/n_q459w55tq87t1kfxnp8hrr0000gn/T/ipykernel_15634/4262826452.py:525: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  x_key = np.array(x, dtype='f').tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15300, 68, 8)\n",
      "(3824, 68, 8)\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 18:50:29.822388: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/479 [==============================] - ETA: 0s - loss: 4.0879"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 18:50:44.996419: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/479 [==============================] - 19s 34ms/step - loss: 4.0879 - val_loss: 3.9164\n",
      "Epoch 2/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.8796 - val_loss: 1.7950\n",
      "Epoch 3/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 1.3570 - val_loss: 1.5424\n",
      "Epoch 4/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.3203 - val_loss: 1.5285\n",
      "Epoch 5/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.2997 - val_loss: 1.5294\n",
      "Epoch 6/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 1.2873 - val_loss: 1.5361\n",
      "Epoch 7/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 1.2603 - val_loss: 1.5113\n",
      "Epoch 8/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.2420 - val_loss: 1.5152\n",
      "Epoch 9/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.2223 - val_loss: 1.4883\n",
      "Epoch 10/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.2039 - val_loss: 1.4819\n",
      "Epoch 11/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.1838 - val_loss: 1.4696\n",
      "Epoch 12/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.1708 - val_loss: 1.4871\n",
      "Epoch 13/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.1533 - val_loss: 1.4849\n",
      "Epoch 14/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.1348 - val_loss: 1.4698\n",
      "Epoch 15/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 1.1164 - val_loss: 1.4874\n",
      "Epoch 16/1000\n",
      "479/479 [==============================] - 12s 26ms/step - loss: 1.1049 - val_loss: 1.4532\n",
      "Epoch 17/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 1.0875 - val_loss: 1.4686\n",
      "Epoch 18/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 1.0708 - val_loss: 1.4461\n",
      "Epoch 19/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 1.0565 - val_loss: 1.4740\n",
      "Epoch 20/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.0477 - val_loss: 1.4391\n",
      "Epoch 21/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.0317 - val_loss: 1.4369\n",
      "Epoch 22/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.0173 - val_loss: 1.4158\n",
      "Epoch 23/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 1.0023 - val_loss: 1.4368\n",
      "Epoch 24/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.9840 - val_loss: 1.4192\n",
      "Epoch 25/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.9726 - val_loss: 1.4365\n",
      "Epoch 26/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.9592 - val_loss: 1.4009\n",
      "Epoch 27/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.9450 - val_loss: 1.4055\n",
      "Epoch 28/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.9328 - val_loss: 1.3866\n",
      "Epoch 29/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.9187 - val_loss: 1.3693\n",
      "Epoch 30/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.9080 - val_loss: 1.3626\n",
      "Epoch 31/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.8911 - val_loss: 1.3916\n",
      "Epoch 32/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.8797 - val_loss: 1.3476\n",
      "Epoch 33/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.8629 - val_loss: 1.3380\n",
      "Epoch 34/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.8531 - val_loss: 1.3276\n",
      "Epoch 35/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.8392 - val_loss: 1.3179\n",
      "Epoch 36/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.8284 - val_loss: 1.2930\n",
      "Epoch 37/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.8124 - val_loss: 1.2502\n",
      "Epoch 38/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.8008 - val_loss: 1.2393\n",
      "Epoch 39/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.7843 - val_loss: 1.2739\n",
      "Epoch 40/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.7726 - val_loss: 1.2589\n",
      "Epoch 41/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7684 - val_loss: 1.2561\n",
      "Epoch 42/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.7570 - val_loss: 1.2313\n",
      "Epoch 43/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.7440 - val_loss: 1.2060\n",
      "Epoch 44/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7335 - val_loss: 1.1834\n",
      "Epoch 45/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7257 - val_loss: 1.2361\n",
      "Epoch 46/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7128 - val_loss: 1.2338\n",
      "Epoch 47/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7064 - val_loss: 1.1551\n",
      "Epoch 48/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.7011 - val_loss: 1.2033\n",
      "Epoch 49/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.6926 - val_loss: 1.1377\n",
      "Epoch 50/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.6788 - val_loss: 1.1562\n",
      "Epoch 51/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.6726 - val_loss: 1.1596\n",
      "Epoch 52/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.6635 - val_loss: 1.1335\n",
      "Epoch 53/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.6557 - val_loss: 1.1037\n",
      "Epoch 54/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.6514 - val_loss: 1.0660\n",
      "Epoch 55/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.6446 - val_loss: 1.1138\n",
      "Epoch 56/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.6380 - val_loss: 1.1158\n",
      "Epoch 57/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.6327 - val_loss: 1.0843\n",
      "Epoch 58/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.6260 - val_loss: 1.0791\n",
      "Epoch 59/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.6183 - val_loss: 1.0792\n",
      "Epoch 60/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.6134 - val_loss: 1.0715\n",
      "Epoch 61/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.6080 - val_loss: 0.9978\n",
      "Epoch 62/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.6025 - val_loss: 1.0243\n",
      "Epoch 63/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.5973 - val_loss: 1.0355\n",
      "Epoch 64/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.5933 - val_loss: 1.0026\n",
      "Epoch 65/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.5911 - val_loss: 1.0180\n",
      "Epoch 66/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5847 - val_loss: 0.9904\n",
      "Epoch 67/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5828 - val_loss: 0.9878\n",
      "Epoch 68/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.5751 - val_loss: 0.9765\n",
      "Epoch 69/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.5685 - val_loss: 0.9781\n",
      "Epoch 70/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5663 - val_loss: 0.9376\n",
      "Epoch 71/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5623 - val_loss: 0.9738\n",
      "Epoch 72/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5604 - val_loss: 0.9708\n",
      "Epoch 73/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5551 - val_loss: 0.9104\n",
      "Epoch 74/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5524 - val_loss: 0.9231\n",
      "Epoch 75/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.5498 - val_loss: 0.9351\n",
      "Epoch 76/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5452 - val_loss: 0.9144\n",
      "Epoch 77/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5394 - val_loss: 0.9350\n",
      "Epoch 78/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5397 - val_loss: 0.9160\n",
      "Epoch 79/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5359 - val_loss: 0.8892\n",
      "Epoch 80/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5332 - val_loss: 0.9223\n",
      "Epoch 81/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.5277 - val_loss: 0.8914\n",
      "Epoch 82/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5276 - val_loss: 0.9090\n",
      "Epoch 83/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.5238 - val_loss: 0.8740\n",
      "Epoch 84/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5212 - val_loss: 0.8723\n",
      "Epoch 85/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5226 - val_loss: 0.8380\n",
      "Epoch 86/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5180 - val_loss: 0.8590\n",
      "Epoch 87/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5157 - val_loss: 0.8755\n",
      "Epoch 88/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5150 - val_loss: 0.8507\n",
      "Epoch 89/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5114 - val_loss: 0.8766\n",
      "Epoch 90/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5076 - val_loss: 0.8335\n",
      "Epoch 91/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.5088 - val_loss: 0.8551\n",
      "Epoch 92/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5036 - val_loss: 0.8284\n",
      "Epoch 93/1000\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.5023 - val_loss: 0.8191\n",
      "Epoch 94/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.5009 - val_loss: 0.8456\n",
      "Epoch 95/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4983 - val_loss: 0.8299\n",
      "Epoch 96/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4944 - val_loss: 0.8296\n",
      "Epoch 97/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4967 - val_loss: 0.8447\n",
      "Epoch 98/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4929 - val_loss: 0.8429\n",
      "Epoch 99/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4923 - val_loss: 0.8247\n",
      "Epoch 100/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4927 - val_loss: 0.8167\n",
      "Epoch 101/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4905 - val_loss: 0.8116\n",
      "Epoch 102/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4893 - val_loss: 0.8169\n",
      "Epoch 103/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4843 - val_loss: 0.7979\n",
      "Epoch 104/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4865 - val_loss: 0.8087\n",
      "Epoch 105/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4827 - val_loss: 0.8029\n",
      "Epoch 106/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4795 - val_loss: 0.8114\n",
      "Epoch 107/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.4851 - val_loss: 0.8196\n",
      "Epoch 108/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4824 - val_loss: 0.7809\n",
      "Epoch 109/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4769 - val_loss: 0.8087\n",
      "Epoch 110/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4805 - val_loss: 0.7763\n",
      "Epoch 111/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4767 - val_loss: 0.7953\n",
      "Epoch 112/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4772 - val_loss: 0.7917\n",
      "Epoch 113/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4754 - val_loss: 0.7983\n",
      "Epoch 114/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4747 - val_loss: 0.7933\n",
      "Epoch 115/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4731 - val_loss: 0.7792\n",
      "Epoch 116/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4714 - val_loss: 0.7738\n",
      "Epoch 117/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4717 - val_loss: 0.7869\n",
      "Epoch 118/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4707 - val_loss: 0.7646\n",
      "Epoch 119/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4713 - val_loss: 0.7693\n",
      "Epoch 120/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4678 - val_loss: 0.7534\n",
      "Epoch 121/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4675 - val_loss: 0.7564\n",
      "Epoch 122/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4657 - val_loss: 0.7677\n",
      "Epoch 123/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4671 - val_loss: 0.7368\n",
      "Epoch 124/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4671 - val_loss: 0.7526\n",
      "Epoch 125/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4684 - val_loss: 0.7434\n",
      "Epoch 126/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4661 - val_loss: 0.7433\n",
      "Epoch 127/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4648 - val_loss: 0.7460\n",
      "Epoch 128/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4642 - val_loss: 0.7539\n",
      "Epoch 129/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4620 - val_loss: 0.7334\n",
      "Epoch 130/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4607 - val_loss: 0.7353\n",
      "Epoch 131/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4612 - val_loss: 0.7583\n",
      "Epoch 132/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4631 - val_loss: 0.7475\n",
      "Epoch 133/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4577 - val_loss: 0.7421\n",
      "Epoch 134/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4568 - val_loss: 0.7403\n",
      "Epoch 135/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4584 - val_loss: 0.7578\n",
      "Epoch 136/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4559 - val_loss: 0.7400\n",
      "Epoch 137/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4556 - val_loss: 0.7428\n",
      "Epoch 138/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4569 - val_loss: 0.7257\n",
      "Epoch 139/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4534 - val_loss: 0.7244\n",
      "Epoch 140/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4557 - val_loss: 0.7431\n",
      "Epoch 141/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4541 - val_loss: 0.7308\n",
      "Epoch 142/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4528 - val_loss: 0.7504\n",
      "Epoch 143/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4583 - val_loss: 0.7648\n",
      "Epoch 144/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4519 - val_loss: 0.7304\n",
      "Epoch 145/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4540 - val_loss: 0.7329\n",
      "Epoch 146/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4509 - val_loss: 0.7238\n",
      "Epoch 147/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4499 - val_loss: 0.7104\n",
      "Epoch 148/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4509 - val_loss: 0.7363\n",
      "Epoch 149/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4504 - val_loss: 0.7304\n",
      "Epoch 150/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4471 - val_loss: 0.7102\n",
      "Epoch 151/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4500 - val_loss: 0.7154\n",
      "Epoch 152/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4464 - val_loss: 0.7149\n",
      "Epoch 153/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4471 - val_loss: 0.7050\n",
      "Epoch 154/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4457 - val_loss: 0.7294\n",
      "Epoch 155/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.4448 - val_loss: 0.7181\n",
      "Epoch 156/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4485 - val_loss: 0.7330\n",
      "Epoch 157/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.4442 - val_loss: 0.7120\n",
      "Epoch 158/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4438 - val_loss: 0.7003\n",
      "Epoch 159/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4439 - val_loss: 0.7033\n",
      "Epoch 160/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4421 - val_loss: 0.7083\n",
      "Epoch 161/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4417 - val_loss: 0.7347\n",
      "Epoch 162/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4413 - val_loss: 0.6939\n",
      "Epoch 163/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4411 - val_loss: 0.7046\n",
      "Epoch 164/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4415 - val_loss: 0.7063\n",
      "Epoch 165/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4390 - val_loss: 0.6919\n",
      "Epoch 166/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4395 - val_loss: 0.7107\n",
      "Epoch 167/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4412 - val_loss: 0.7009\n",
      "Epoch 168/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4387 - val_loss: 0.7303\n",
      "Epoch 169/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4365 - val_loss: 0.7119\n",
      "Epoch 170/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4397 - val_loss: 0.6969\n",
      "Epoch 171/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4379 - val_loss: 0.6947\n",
      "Epoch 172/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 0.4373 - val_loss: 0.7113\n",
      "Epoch 173/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.4369 - val_loss: 0.7115\n",
      "Epoch 174/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4367 - val_loss: 0.7069\n",
      "Epoch 175/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4381 - val_loss: 0.6952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 19:28:11.055346: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 4s 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/n_q459w55tq87t1kfxnp8hrr0000gn/T/ipykernel_15634/4262826452.py:448: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  input_feats_key = np.array(input_feats, dtype='f').tostring()\n",
      "/var/folders/8c/n_q459w55tq87t1kfxnp8hrr0000gn/T/ipykernel_15634/4262826452.py:525: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  x_key = np.array(x, dtype='f').tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15300, 68, 8)\n",
      "(3824, 68, 8)\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 19:28:18.108361: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/479 [==============================] - ETA: 0s - loss: 4.0776"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 19:28:35.585400: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/479 [==============================] - 22s 40ms/step - loss: 4.0776 - val_loss: 3.7765\n",
      "Epoch 2/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.8558 - val_loss: 1.6846\n",
      "Epoch 3/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.3455 - val_loss: 1.4839\n",
      "Epoch 4/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.3319 - val_loss: 1.4728\n",
      "Epoch 5/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.3032 - val_loss: 1.4655\n",
      "Epoch 6/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.2848 - val_loss: 1.4836\n",
      "Epoch 7/1000\n",
      "479/479 [==============================] - 14s 30ms/step - loss: 1.2701 - val_loss: 1.4648\n",
      "Epoch 8/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.2454 - val_loss: 1.4710\n",
      "Epoch 9/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.2313 - val_loss: 1.4343\n",
      "Epoch 10/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.2066 - val_loss: 1.4343\n",
      "Epoch 11/1000\n",
      "479/479 [==============================] - 13s 27ms/step - loss: 1.1968 - val_loss: 1.4217\n",
      "Epoch 12/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.1808 - val_loss: 1.4249\n",
      "Epoch 13/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.1585 - val_loss: 1.4024\n",
      "Epoch 14/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.1559 - val_loss: 1.4021\n",
      "Epoch 15/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.1418 - val_loss: 1.3816\n",
      "Epoch 16/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.1282 - val_loss: 1.3931\n",
      "Epoch 17/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.1175 - val_loss: 1.3827\n",
      "Epoch 18/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 1.1002 - val_loss: 1.3626\n",
      "Epoch 19/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.0913 - val_loss: 1.3602\n",
      "Epoch 20/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.0834 - val_loss: 1.3501\n",
      "Epoch 21/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.0731 - val_loss: 1.3430\n",
      "Epoch 22/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.0580 - val_loss: 1.3439\n",
      "Epoch 23/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.0459 - val_loss: 1.3231\n",
      "Epoch 24/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.0390 - val_loss: 1.3157\n",
      "Epoch 25/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 1.0288 - val_loss: 1.3276\n",
      "Epoch 26/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.0154 - val_loss: 1.3238\n",
      "Epoch 27/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 1.0041 - val_loss: 1.2871\n",
      "Epoch 28/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.9943 - val_loss: 1.3067\n",
      "Epoch 29/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.9802 - val_loss: 1.2989\n",
      "Epoch 30/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.9738 - val_loss: 1.2637\n",
      "Epoch 31/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.9604 - val_loss: 1.2606\n",
      "Epoch 32/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.9499 - val_loss: 1.2709\n",
      "Epoch 33/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.9370 - val_loss: 1.2081\n",
      "Epoch 34/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.9238 - val_loss: 1.2409\n",
      "Epoch 35/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.9110 - val_loss: 1.2387\n",
      "Epoch 36/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.9012 - val_loss: 1.1931\n",
      "Epoch 37/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.8855 - val_loss: 1.2211\n",
      "Epoch 38/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.8745 - val_loss: 1.1862\n",
      "Epoch 39/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.8640 - val_loss: 1.2086\n",
      "Epoch 40/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.8455 - val_loss: 1.1478\n",
      "Epoch 41/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.8353 - val_loss: 1.1800\n",
      "Epoch 42/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.8212 - val_loss: 1.2044\n",
      "Epoch 43/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.8115 - val_loss: 1.1501\n",
      "Epoch 44/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.7945 - val_loss: 1.1413\n",
      "Epoch 45/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.7810 - val_loss: 1.1626\n",
      "Epoch 46/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.7698 - val_loss: 1.1136\n",
      "Epoch 47/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.7586 - val_loss: 1.1177\n",
      "Epoch 48/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.7483 - val_loss: 1.0952\n",
      "Epoch 49/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.7331 - val_loss: 1.1058\n",
      "Epoch 50/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.7221 - val_loss: 1.0492\n",
      "Epoch 51/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.7090 - val_loss: 1.0282\n",
      "Epoch 52/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.7015 - val_loss: 1.0426\n",
      "Epoch 53/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.6895 - val_loss: 1.0131\n",
      "Epoch 54/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.6749 - val_loss: 1.0445\n",
      "Epoch 55/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.6673 - val_loss: 1.0098\n",
      "Epoch 56/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.6595 - val_loss: 1.0193\n",
      "Epoch 57/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.6485 - val_loss: 1.0006\n",
      "Epoch 58/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.6374 - val_loss: 0.9658\n",
      "Epoch 59/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.6301 - val_loss: 0.9738\n",
      "Epoch 60/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.6214 - val_loss: 0.9595\n",
      "Epoch 61/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.6139 - val_loss: 0.9663\n",
      "Epoch 62/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.6092 - val_loss: 0.9338\n",
      "Epoch 63/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.6017 - val_loss: 0.9276\n",
      "Epoch 64/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5915 - val_loss: 0.9201\n",
      "Epoch 65/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5872 - val_loss: 0.9264\n",
      "Epoch 66/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5772 - val_loss: 0.9140\n",
      "Epoch 67/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5775 - val_loss: 0.8634\n",
      "Epoch 68/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5709 - val_loss: 0.8756\n",
      "Epoch 69/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5652 - val_loss: 0.8638\n",
      "Epoch 70/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5610 - val_loss: 0.8653\n",
      "Epoch 71/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5573 - val_loss: 0.8942\n",
      "Epoch 72/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.5547 - val_loss: 0.8449\n",
      "Epoch 73/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5499 - val_loss: 0.8397\n",
      "Epoch 74/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5448 - val_loss: 0.8430\n",
      "Epoch 75/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.5393 - val_loss: 0.8696\n",
      "Epoch 76/1000\n",
      "479/479 [==============================] - 14s 30ms/step - loss: 0.5349 - val_loss: 0.8220\n",
      "Epoch 77/1000\n",
      "479/479 [==============================] - 14s 30ms/step - loss: 0.5340 - val_loss: 0.8196\n",
      "Epoch 78/1000\n",
      "479/479 [==============================] - 14s 30ms/step - loss: 0.5307 - val_loss: 0.8080\n",
      "Epoch 79/1000\n",
      "479/479 [==============================] - 14s 30ms/step - loss: 0.5276 - val_loss: 0.7884\n",
      "Epoch 80/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.5247 - val_loss: 0.8003\n",
      "Epoch 81/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5201 - val_loss: 0.8204\n",
      "Epoch 82/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5210 - val_loss: 0.8020\n",
      "Epoch 83/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.5173 - val_loss: 0.7783\n",
      "Epoch 84/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5163 - val_loss: 0.7817\n",
      "Epoch 85/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.5170 - val_loss: 0.7855\n",
      "Epoch 86/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5097 - val_loss: 0.7627\n",
      "Epoch 87/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.5103 - val_loss: 0.7804\n",
      "Epoch 88/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5060 - val_loss: 0.7664\n",
      "Epoch 89/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5055 - val_loss: 0.7654\n",
      "Epoch 90/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.5059 - val_loss: 0.7614\n",
      "Epoch 91/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.5010 - val_loss: 0.7495\n",
      "Epoch 92/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4973 - val_loss: 0.7491\n",
      "Epoch 93/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4982 - val_loss: 0.7407\n",
      "Epoch 94/1000\n",
      "479/479 [==============================] - 13s 28ms/step - loss: 0.4992 - val_loss: 0.7465\n",
      "Epoch 95/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4968 - val_loss: 0.7563\n",
      "Epoch 96/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4952 - val_loss: 0.7591\n",
      "Epoch 97/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4954 - val_loss: 0.7349\n",
      "Epoch 98/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4919 - val_loss: 0.7368\n",
      "Epoch 99/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4945 - val_loss: 0.7526\n",
      "Epoch 100/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4886 - val_loss: 0.7506\n",
      "Epoch 101/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4919 - val_loss: 0.7325\n",
      "Epoch 102/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4875 - val_loss: 0.7321\n",
      "Epoch 103/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4876 - val_loss: 0.7332\n",
      "Epoch 104/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4883 - val_loss: 0.7394\n",
      "Epoch 105/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4840 - val_loss: 0.7506\n",
      "Epoch 106/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4828 - val_loss: 0.7333\n",
      "Epoch 107/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4852 - val_loss: 0.7375\n",
      "Epoch 108/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4827 - val_loss: 0.7193\n",
      "Epoch 109/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4801 - val_loss: 0.7106\n",
      "Epoch 110/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4834 - val_loss: 0.7092\n",
      "Epoch 111/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4815 - val_loss: 0.7247\n",
      "Epoch 112/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4796 - val_loss: 0.7356\n",
      "Epoch 113/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4811 - val_loss: 0.7227\n",
      "Epoch 114/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4771 - val_loss: 0.7232\n",
      "Epoch 115/1000\n",
      "479/479 [==============================] - 14s 28ms/step - loss: 0.4743 - val_loss: 0.7147\n",
      "Epoch 116/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4767 - val_loss: 0.7243\n",
      "Epoch 117/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4792 - val_loss: 0.7206\n",
      "Epoch 118/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4781 - val_loss: 0.7173\n",
      "Epoch 119/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4720 - val_loss: 0.7043\n",
      "Epoch 120/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4724 - val_loss: 0.7058\n",
      "Epoch 121/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4741 - val_loss: 0.7172\n",
      "Epoch 122/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4708 - val_loss: 0.7112\n",
      "Epoch 123/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4724 - val_loss: 0.7014\n",
      "Epoch 124/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4717 - val_loss: 0.7072\n",
      "Epoch 125/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4716 - val_loss: 0.6963\n",
      "Epoch 126/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4695 - val_loss: 0.7093\n",
      "Epoch 127/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4696 - val_loss: 0.7066\n",
      "Epoch 128/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4679 - val_loss: 0.6972\n",
      "Epoch 129/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4687 - val_loss: 0.7005\n",
      "Epoch 130/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4661 - val_loss: 0.7090\n",
      "Epoch 131/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4626 - val_loss: 0.6941\n",
      "Epoch 132/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4653 - val_loss: 0.6976\n",
      "Epoch 133/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4649 - val_loss: 0.6870\n",
      "Epoch 134/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4650 - val_loss: 0.7007\n",
      "Epoch 135/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4653 - val_loss: 0.7084\n",
      "Epoch 136/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4643 - val_loss: 0.7051\n",
      "Epoch 137/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4633 - val_loss: 0.7077\n",
      "Epoch 138/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4619 - val_loss: 0.7044\n",
      "Epoch 139/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4633 - val_loss: 0.6960\n",
      "Epoch 140/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4636 - val_loss: 0.7047\n",
      "Epoch 141/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4631 - val_loss: 0.6958\n",
      "Epoch 142/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4623 - val_loss: 0.6956\n",
      "Epoch 143/1000\n",
      "479/479 [==============================] - 14s 29ms/step - loss: 0.4620 - val_loss: 0.6963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 20:00:57.204186: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 3s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "#Temporary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr\n",
    "import keras\n",
    "\n",
    "epochs =1000\n",
    "es = tfk.callbacks.EarlyStopping(monitor='val_loss',mode='min', patience=10)\n",
    "\n",
    "loss = 'mse'\n",
    "batch_size=32\n",
    "\n",
    "seeds = [1221,4546,9999]\n",
    "\n",
    "adapt_mse_save = []\n",
    "adapt_r2_save = []\n",
    "adapt_spearman_save = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    #Split the data\n",
    "    # sandstorm_train,sandstorm_test,adapt_train,adapt_test,ppm_train,ppm_test,guide_ppm_train,guide_ppm_test,y_train,y_test = train_test_split(guides_and_targets,adapt_inputs,ppms,guide_ppms,values,test_size=0.2)\n",
    "    # print(sandstorm_train.shape)\n",
    "    # print(adapt_train.shape)\n",
    "    \n",
    "    data_parser = Cas13ActivityParser\n",
    "\n",
    "    tmp = data_parser(\n",
    "            context_nt=20,\n",
    "            split=(0.8, 0,0.2),\n",
    "            shuffle_seed=seeds[i],\n",
    "            stratify_by_pos=False,\n",
    "            subset='exp',\n",
    "            use_median_measurement=True)\n",
    "\n",
    "    tmp.read()\n",
    "# print(help(data_parser))\n",
    "\n",
    "    adapt_train,y_train=  tmp.train_set()\n",
    "\n",
    "    adapt_test,y_test = tmp.test_set()\n",
    "    \n",
    "    print(adapt_train.shape)\n",
    "    print(adapt_test.shape)\n",
    "    \n",
    "#     print(adapt_train.shape)\n",
    "    \n",
    "#     seq_train = adapt_train[:,:,:].transpose(0,2,1)\n",
    "#     seq_test = adapt_test[:,:,:].transpose(0,2,1)\n",
    "#     print(seq_train.shape)\n",
    "    \n",
    "#     ppm_train = GA_util.prototype_ppms_fast(seq_train[:,:4,:])\n",
    "#     ppm_test = GA_util.prototype_ppms_fast(seq_test[:,:4,:])\n",
    "\n",
    "    adapt = construct_model(params=params,shape=(None,48,8))\n",
    "    # print(adapt.summary())\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "\n",
    "    adapt.compile(loss='mse',optimizer=optimizer)\n",
    "\n",
    "\n",
    "    es = tfk.callbacks.EarlyStopping(monitor='val_loss',\n",
    "            mode='min', patience=10)\n",
    "\n",
    "    hist_adapt = adapt.fit(adapt_train,y_train,epochs=epochs,validation_data=[adapt_test,y_test],callbacks=es)\n",
    "    adapt_mse = hist_adapt.history['val_loss'][-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    adapt_preds = adapt.predict(adapt_test)\n",
    "    \n",
    "    adapt_r2 = r2_score(y_test,adapt_preds)\n",
    "    adapt_spearman = spearmanr(y_test,adapt_preds)[0]\n",
    "    \n",
    "    adapt_mse_save.append(adapt_mse)\n",
    "    adapt_r2_save.append(adapt_r2)\n",
    "    adapt_spearman_save.append(adapt_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb121dc6-94df-45a5-9a3a-8e971ded3762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8197777809823622, 0.8249473340339805, 0.8144842894988256]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_spearman_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d72070d-704e-40d9-9536-4f420f4c30b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7076068728438761, 0.7282203579718209, 0.7186757001423293]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapt_spearman_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6eef01b-36a6-4339-9826-642b5b0e9f00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21769\n",
      "51243\n"
     ]
    }
   ],
   "source": [
    "joint_params = np.sum([np.prod(v.get_shape()) for v in joint_model.trainable_weights])\n",
    "print(joint_params)\n",
    "\n",
    "adapt_params = np.sum([np.prod(v.get_shape()) for v in adapt.trainable_weights])\n",
    "print(adapt_params)\n",
    "adapt_params = 51243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35613302-a71d-480a-b3c4-93f3f068a996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 675x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "# mse, r2_on, r2_off, spearman_on, spearman_off, parameters\n",
    "joint_model_bars = [np.mean(joint_mse_save),\n",
    "                    np.mean(joint_r2_save),\n",
    "                    np.mean(joint_spearman_save),\n",
    "                    joint_params/adapt_params]\n",
    "\n",
    "\n",
    "joint_model_variances = [np.std(joint_mse_save),\n",
    "                         np.std(joint_r2_save),\n",
    "                         np.std(joint_spearman_save),\n",
    "                        0]\n",
    "\n",
    "adapt_bars = [np.mean(adapt_mse_save),\n",
    "                    np.mean(adapt_r2_save),\n",
    "                    np.mean(adapt_spearman_save),\n",
    "                    adapt_params/adapt_params]\n",
    "\n",
    "adapt_variances = [np.std(adapt_mse_save),\n",
    "                         np.std(adapt_r2_save),\n",
    "                         np.std(adapt_spearman_save),\n",
    "                         0]\n",
    "\n",
    "# joint_model_bars= np.load('stat_tests/joint_model_cas_metrics.npy')\n",
    "# joint_model_variances = np.load('stat_tests/joint_model_cas_variances.npy')\n",
    "\n",
    "# adapt_bars = np.load('stat_tests/adapt_metrics.npy')\n",
    "# adapt_variances=np.load('stat_tests/adapt_variances.npy')\n",
    "\n",
    "barwidth = 0.3\n",
    "RED = '#C62626'\n",
    "BLUE = '#2E9AD6'\n",
    "\n",
    "plt.rcParams.update({'font.size': 12,'font.family':'Helvetica'})\n",
    "plt.rcParams['mathtext.fontset'] = 'custom'\n",
    "plt.rcParams['mathtext.rm'] = 'Helvetica'\n",
    "plt.rcParams['mathtext.it'] = 'Helvetica:italic'\n",
    "plt.rcParams['mathtext.bf'] = 'Helvetica:bold'\n",
    "r1 = np.arange(len(joint_model_bars))\n",
    "r2 = [x + barwidth for x in r1]\n",
    "\n",
    "plt.figure(figsize=(6.75,3))\n",
    "plt.title('Cas13a Target Predictions')\n",
    "plt.bar(r1, adapt_bars, width = barwidth, color = BLUE, yerr=adapt_variances, capsize=5, label='ADAPT',edgecolor='black',linewidth=2)\n",
    "plt.bar(r2, joint_model_bars, width = barwidth, color = RED, yerr=joint_model_variances, capsize=5, label='SANDSTORM',edgecolor='black',linewidth=2)\n",
    "plt.xticks([r + (barwidth/2) for r in range(len(joint_model_bars))], ['MSE', r'R$^{\\rm 2}$','Spearman','Parameter Fraction'],fontdict={'family':'Helvetica'})\n",
    "plt.subplot().spines['right'].set_visible(False)\n",
    "plt.subplot().spines['top'].set_visible(False)\n",
    "plt.subplot().spines['left'].set_linewidth(2.0)\n",
    "plt.subplot().spines['bottom'].set_linewidth(2.0)\n",
    "plt.subplot().xaxis.set_tick_params(width=1.5)\n",
    "plt.subplot().yaxis.set_tick_params(width=1.5)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.12, 0.5),\n",
    "          fancybox=True, shadow=True, ncol=1)\n",
    "# plt.savefig('../figures/SHERLOCK Predictor edgecolor.png',dpi=500,pad_inches=0.5,bbox_inches='tight')\n",
    "# plt.savefig('../figures/SHERLOCK Predictor edgecolor.svg',transparent=True,pad_inches=0.5,bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87140d96-c00a-4c11-abba-6e673de6589d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7535085678100586, 0.43392382688099956, 0.7202063167365912, 1.0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapt_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd79aab5-293e-4c92-9058-ebcafbeb41a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50051842, 0.57208042, 0.7867413 , 0.424819  ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_model_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed9e519-5ca6-4012-8827-c66b56b85aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ttest_indResult(statistic=-124.34797477890974, pvalue=2.5084714387401365e-08),\n",
       " Ttest_indResult(statistic=15.750683887239209, pvalue=9.492315990547379e-05),\n",
       " Ttest_indResult(statistic=19.371884833329457, pvalue=4.185888265920512e-05)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "joint_model_bars\n",
    "joint_model_variances\n",
    "\n",
    "save = []\n",
    "for i in range(len(joint_model_bars)-1):\n",
    "    save.append(scipy.stats.ttest_ind_from_stats(joint_model_bars[i], \n",
    "                                     joint_model_variances[i], \n",
    "                                     3,\n",
    "                                     adapt_bars[i],\n",
    "                                     adapt_variances[i],\n",
    "                                     3, equal_var=True, alternative='two-sided'))\n",
    "    \n",
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a183b051-3ec0-4cef-976f-9cf3fdfa6180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('stat_tests/joint_model_cas_metrics.npy',joint_model_bars)\n",
    "np.save('stat_tests/joint_model_cas_variances.npy',joint_model_variances)\n",
    "\n",
    "np.save('stat_tests/adapt_metrics.npy',adapt_bars)\n",
    "np.save('stat_tests/adapt_variances.npy',adapt_variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d8a693a-1249-428b-a01a-02292c62fcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5005184213320414,\n",
       " 0.5720804243830323,\n",
       " 0.7867412959899465,\n",
       " 0.4248189996682474]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_model_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2154948-36bb-451f-af9a-cdee67ec0dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
